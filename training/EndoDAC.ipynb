{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cell1_setup",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ebcecdf7-8180-4e12-8152-23b6ac31ef7e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Cloning into 'EndoDAC'...\n",
            "remote: Enumerating objects: 235, done.\u001b[K\n",
            "remote: Counting objects: 100% (116/116), done.\u001b[K\n",
            "remote: Compressing objects: 100% (113/113), done.\u001b[K\n",
            "remote: Total 235 (delta 60), reused 2 (delta 0), pack-reused 119 (from 1)\u001b[K\n",
            "Receiving objects: 100% (235/235), 1.43 MiB | 19.22 MiB/s, done.\n",
            "Resolving deltas: 100% (85/85), done.\n",
            "   Skipping: --extra-index-url https://download.pytorch.org/whl/cu117\n",
            "   Skipping: torch==2.0.0\n",
            "   Skipping: torchvision==0.15.0\n",
            "   Skipping: torchmetrics==0.10.3\n",
            "   Skipping: xformers==0.0.18\n",
            "âœ… Patched requirements.txt\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m122.9/122.9 MB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m50.2/50.2 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m42.2/42.2 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m87.2/87.2 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for fvcore (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for iopath (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m8.2/8.2 MB\u001b[0m \u001b[31m102.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m241.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m832.7/832.7 kB\u001b[0m \u001b[31m119.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m138.4/138.4 kB\u001b[0m \u001b[31m68.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m50.8/50.8 kB\u001b[0m \u001b[31m42.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m139.9/139.9 MB\u001b[0m \u001b[31m92.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m168.4/168.4 MB\u001b[0m \u001b[31m77.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m58.1/58.1 MB\u001b[0m \u001b[31m111.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m128.2/128.2 MB\u001b[0m \u001b[31m74.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m204.1/204.1 MB\u001b[0m \u001b[31m101.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m855.0/855.0 kB\u001b[0m \u001b[31m226.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m318.5/318.5 MB\u001b[0m \u001b[31m60.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m133.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m8.8/8.8 MB\u001b[0m \u001b[31m118.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m445.9/445.9 MB\u001b[0m \u001b[31m63.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m8.8/8.8 MB\u001b[0m \u001b[31m35.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m27.7/27.7 MB\u001b[0m \u001b[31m214.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.0/1.0 GB\u001b[0m \u001b[31m32.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m18.9/18.9 MB\u001b[0m \u001b[31m218.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m95.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m201.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m681.9/681.9 kB\u001b[0m \u001b[31m203.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m58.6/58.6 MB\u001b[0m \u001b[31m126.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m507.5/507.5 kB\u001b[0m \u001b[31m182.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m76.0/76.0 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m447.7/447.7 MB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m49.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m54.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m99.7/99.7 MB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m7.9/7.9 MB\u001b[0m \u001b[31m142.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m139.8/139.8 kB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m417.9/417.9 MB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m8.6/8.6 MB\u001b[0m \u001b[31m17.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m577.0/577.0 kB\u001b[0m \u001b[31m37.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m74.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m64.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m27.6/27.6 MB\u001b[0m \u001b[31m66.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m47.5/47.5 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "cuml-cu12 25.10.0 requires cuda-python<13.0a0,>=12.9.2, but you have cuda-python 11.8.7 which is incompatible.\n",
            "cuml-cu12 25.10.0 requires dask-cuda==25.10.*, but you have dask-cuda 25.6.0 which is incompatible.\n",
            "cuml-cu12 25.10.0 requires numba-cuda[cu12]<0.20.0a0,>=0.19.1, but you have numba-cuda 0.11.0 which is incompatible.\n",
            "cuml-cu12 25.10.0 requires rapids-dask-dependency==25.10.*, but you have rapids-dask-dependency 25.6.0 which is incompatible.\n",
            "distributed-ucxx-cu12 0.46.0 requires numba-cuda[cu12]<0.20.0a0,>=0.19.1, but you have numba-cuda 0.11.0 which is incompatible.\n",
            "distributed-ucxx-cu12 0.46.0 requires rapids-dask-dependency==25.10.*, but you have rapids-dask-dependency 25.6.0 which is incompatible.\n",
            "dask-cudf-cu12 25.10.0 requires rapids-dask-dependency==25.10.*, but you have rapids-dask-dependency 25.6.0 which is incompatible.\n",
            "raft-dask-cu12 25.10.0 requires dask-cuda==25.10.*, but you have dask-cuda 25.6.0 which is incompatible.\n",
            "raft-dask-cu12 25.10.0 requires rapids-dask-dependency==25.10.*, but you have rapids-dask-dependency 25.6.0 which is incompatible.\n",
            "pylibraft-cu12 25.10.0 requires cuda-python<13.0a0,>=12.9.2, but you have cuda-python 11.8.7 which is incompatible.\n",
            "cudf-cu12 25.10.0 requires cuda-python<13.0a0,>=12.9.2, but you have cuda-python 11.8.7 which is incompatible.\n",
            "cudf-cu12 25.10.0 requires numba-cuda[cu12]<0.20.0a0,>=0.19.1, but you have numba-cuda 0.11.0 which is incompatible.\n",
            "pylibcudf-cu12 25.10.0 requires cuda-python<13.0a0,>=12.9.2, but you have cuda-python 11.8.7 which is incompatible.\n",
            "ucxx-cu12 0.46.0 requires numba-cuda[cu12]<0.20.0a0,>=0.19.1, but you have numba-cuda 0.11.0 which is incompatible.\n",
            "rmm-cu12 25.10.0 requires cuda-python<13.0a0,>=12.9.2, but you have cuda-python 11.8.7 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mâœ… Environment setup hoÃ n táº¥t!\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "import sys\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "!git clone https://github.com/BeileiCui/EndoDAC\n",
        "\n",
        "req_path = \"/content/EndoDAC/requirements.txt\"\n",
        "if os.path.exists(req_path):\n",
        "    with open(req_path, \"r\") as f:\n",
        "        lines = f.readlines()\n",
        "    with open(req_path, \"w\") as f:\n",
        "        for line in lines:\n",
        "            if \"torch\" in line or \"torchvision\" in line or \"xformers\" in line:\n",
        "                print(f\"   Skipping: {line.strip()}\")\n",
        "                continue\n",
        "            f.write(line)\n",
        "\n",
        "!pip install -q xformers --no-deps\n",
        "!pip install -q tensorboardX timm gdown fvcore\n",
        "!pip install -q -r /content/EndoDAC/requirements.txt\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import glob\n",
        "import zipfile\n",
        "import gdown\n",
        "from tqdm import tqdm\n",
        "\n",
        "repo_path = \"/content/EndoDAC\"\n",
        "\n",
        "# Pretrained Model\n",
        "pretrained_dir = os.path.join(repo_path, \"pretrained_model\")\n",
        "pretrained_file = os.path.join(pretrained_dir, \"depth_anything_vitb14.pth\")\n",
        "\n",
        "if not os.path.exists(pretrained_file):\n",
        "    print(\"Loading Depth-Anything pretrained model...\")\n",
        "    os.makedirs(pretrained_dir, exist_ok=True)\n",
        "    file_id = \"163ILZcnz_-IUoIgy1UF_r7PAQBqgDbll\"\n",
        "    url = f\"https://drive.google.com/uc?id={file_id}\"\n",
        "    gdown.download(url, pretrained_file, quiet=False)\n",
        "    print(f\" Pretrained model: {pretrained_file}\")\n",
        "else:\n",
        "    print(f\" Pretrained model exists\")\n",
        "\n",
        "train_zip_path = \"/content/drive/MyDrive/3D_reconstruction/datasets/FrameBufferS2S8.zip\"\n",
        "training_data_root = \"/content/training_data_simcol\"\n",
        "\n",
        "if not os.path.exists(training_data_root):\n",
        "    print(f\"â³ Äang giáº£i nÃ©n TRAINING DATA tá»« {train_zip_path}...\")\n",
        "    os.makedirs(training_data_root, exist_ok=True)\n",
        "\n",
        "    with zipfile.ZipFile(train_zip_path, 'r') as zip_ref:\n",
        "        zip_ref.extractall(training_data_root)\n",
        "\n",
        "    subfolders = [f.path for f in os.scandir(training_data_root) if f.is_dir()]\n",
        "    print(f\"ğŸ“‚ Sequences: {[os.path.basename(f) for f in subfolders]}\")\n",
        "\n",
        "    # Correct file name: FrameBuffer_0000.png -> 0000000000.png\n",
        "    print(\"â³ Äang chuáº©n hÃ³a tÃªn file...\")\n",
        "    for folder in subfolders:\n",
        "        images = glob.glob(os.path.join(folder, \"*.png\"))\n",
        "        for img_path in tqdm(images, desc=f\"Renaming {os.path.basename(folder)}\"):\n",
        "            folder_path, filename = os.path.split(img_path)\n",
        "            if \"FrameBuffer_\" in filename:\n",
        "                new_name = filename.replace(\"FrameBuffer_\", \"\")\n",
        "                try:\n",
        "                    num = int(new_name.split('.')[0])\n",
        "                    new_filename = \"{:010d}.png\".format(num)\n",
        "                    os.rename(img_path, os.path.join(folder_path, new_filename))\n",
        "                except ValueError:\n",
        "                    pass\n",
        "\n",
        "    print(f\"Data prepared at: {training_data_root}\")\n",
        "else:\n",
        "    print(f\"Data existed: {training_data_root}\")"
      ],
      "metadata": {
        "id": "cell2_data",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fd1af777-8d7e-44d5-932b-7bbde21a9bb6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "â³ Äang táº£i Depth-Anything pretrained model...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=163ILZcnz_-IUoIgy1UF_r7PAQBqgDbll\n",
            "From (redirected): https://drive.google.com/uc?id=163ILZcnz_-IUoIgy1UF_r7PAQBqgDbll&confirm=t&uuid=39937089-18a9-4ff2-b2ef-457f30a3bdd3\n",
            "To: /content/EndoDAC/pretrained_model/depth_anything_vitb14.pth\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 390M/390M [00:03<00:00, 112MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Pretrained model: /content/EndoDAC/pretrained_model/depth_anything_vitb14.pth\n",
            "â³ Äang giáº£i nÃ©n TRAINING DATA tá»« /content/drive/MyDrive/3D_reconstruction/datasets/FrameBufferS2S8.zip...\n",
            "ğŸ“‚ Sequences: ['FrameBuffer_S4', 'FrameBuffer_S5', 'FrameBuffer_S3', 'FrameBuffer_S7', 'FrameBuffer_S8', 'FrameBuffer_S6', 'FrameBuffer_S2']\n",
            "â³ Äang chuáº©n hÃ³a tÃªn file...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Renaming FrameBuffer_S4: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1201/1201 [00:00<00:00, 29604.18it/s]\n",
            "Renaming FrameBuffer_S5: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1201/1201 [00:00<00:00, 28194.12it/s]\n",
            "Renaming FrameBuffer_S3: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1201/1201 [00:00<00:00, 32642.30it/s]\n",
            "Renaming FrameBuffer_S7: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1201/1201 [00:00<00:00, 31837.69it/s]\n",
            "Renaming FrameBuffer_S8: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1201/1201 [00:00<00:00, 29834.93it/s]\n",
            "Renaming FrameBuffer_S6: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1201/1201 [00:00<00:00, 30311.57it/s]\n",
            "Renaming FrameBuffer_S2: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1201/1201 [00:00<00:00, 32518.60it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Dá»¯ liá»‡u sáºµn sÃ ng táº¡i: /content/training_data_simcol\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import glob\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "repo_path = \"/content/EndoDAC\"\n",
        "training_data_root = \"/content/training_data_simcol\"\n",
        "\n",
        "#(utils/layers.py)\n",
        "\n",
        "layers_path = os.path.join(repo_path, \"utils\", \"layers.py\")\n",
        "with open(layers_path, 'r') as f:\n",
        "    l_content = f.read()\n",
        "\n",
        "unsafe_code = \"\"\"    values[invalid] = 0\n",
        "\n",
        "    corresponding_map.scatter_add_(1, indices, values)\"\"\"\n",
        "\n",
        "safe_code = \"\"\"    values[invalid] = 0\n",
        "    indices[invalid] = 0\n",
        "    indices = indices.clamp(0, H * W - 1)\n",
        "\n",
        "    corresponding_map.scatter_add_(1, indices, values)\"\"\"\n",
        "\n",
        "if unsafe_code in l_content:\n",
        "    l_content = l_content.replace(unsafe_code, safe_code)\n",
        "    with open(layers_path, 'w') as f:\n",
        "        f.write(l_content)\n",
        "    print(\"Fix crash at utils/layers.py\")\n",
        "else:\n",
        "    print(\"utils/layers.py had fixed\")\n",
        "\n",
        "# Create SimColDataset CLASS\n",
        "# Normalized K\n",
        "simcol_dataset_content = '''from __future__ import absolute_import, division, print_function\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import PIL.Image as pil\n",
        "\n",
        "from .mono_dataset import MonoDataset\n",
        "\n",
        "class SimColDataset(MonoDataset):\n",
        "    \"\"\"Custom dataset for SimCol3D data\n",
        "    Image format: 0000000000.png (10 digits)\n",
        "    \"\"\"\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super(SimColDataset, self).__init__(*args, **kwargs)\n",
        "\n",
        "        W_orig = 475.0\n",
        "        H_orig = 475.0\n",
        "\n",
        "        # Intrinsic\n",
        "        fx_orig = 227.60416\n",
        "        fy_orig = 237.5\n",
        "        cx_orig = 227.60416\n",
        "        cy_orig = 237.5\n",
        "\n",
        "        fx_norm = fx_orig / W_orig\n",
        "        fy_norm = fy_orig / H_orig\n",
        "        cx_norm = cx_orig / W_orig\n",
        "        cy_norm = cy_orig / H_orig\n",
        "\n",
        "        self.K = np.array([[fx_norm, 0., cx_norm, 0.],\n",
        "                           [0., fy_norm, cy_norm, 0.],\n",
        "                           [0., 0., 1., 0.],\n",
        "                           [0., 0., 0., 1.]], dtype=np.float32)\n",
        "\n",
        "        self.side_map = {\"l\": \"left\", \"r\": \"right\"}\n",
        "\n",
        "    def check_depth(self):\n",
        "        return False\n",
        "\n",
        "    def get_image_path(self, folder, frame_index, side):\n",
        "        f_str = \"{:010d}.png\".format(frame_index)\n",
        "        return os.path.join(self.data_path, folder, f_str)\n",
        "\n",
        "    def get_color(self, folder, frame_index, side, do_flip):\n",
        "        image_path = self.get_image_path(folder, frame_index, side)\n",
        "        color = self.loader(image_path)\n",
        "        if do_flip:\n",
        "            color = color.transpose(pil.FLIP_LEFT_RIGHT)\n",
        "        return color\n",
        "'''\n",
        "\n",
        "with open(os.path.join(repo_path, \"datasets\", \"simcol_dataset.py\"), \"w\") as f:\n",
        "    f.write(simcol_dataset_content)\n",
        "print(\"Created datasets/simcol_dataset.py\")\n",
        "\n",
        "# Update __init__.py\n",
        "init_path = os.path.join(repo_path, \"datasets\", \"__init__.py\")\n",
        "init_content = '''from .mono_dataset import MonoDataset\n",
        "from .scared_dataset import SCAREDDataset, SCAREDRAWDataset\n",
        "from .hamlyn_dataset import HamlynDataset\n",
        "from .c3vd_dataset import C3VDDataset\n",
        "from .simcol_dataset import SimColDataset\n",
        "'''\n",
        "with open(init_path, \"w\") as f:\n",
        "    f.write(init_content)\n",
        "\n",
        "# Update trainer_end_to_end.py\n",
        "trainer_path = os.path.join(repo_path, \"trainer_end_to_end.py\")\n",
        "with open(trainer_path, \"r\") as f:\n",
        "    t_content = f.read()\n",
        "if 'datasets_dict = {\"endovis\": datasets.SCAREDRAWDataset}' in t_content:\n",
        "    t_content = t_content.replace(\n",
        "        'datasets_dict = {\"endovis\": datasets.SCAREDRAWDataset}',\n",
        "        'datasets_dict = {\"endovis\": datasets.SCAREDRAWDataset, \"simcol\": datasets.SimColDataset}'\n",
        "    )\n",
        "    with open(trainer_path, \"w\") as f:\n",
        "        f.write(t_content)\n",
        "print(\" SimColDataset update to Trainer\")\n",
        "\n",
        "# Update options.py\n",
        "options_path = os.path.join(repo_path, \"options.py\")\n",
        "with open(options_path, \"r\") as f:\n",
        "    o_content = f.read()\n",
        "if 'choices=[\"endovis\", \"simcol\"]' not in o_content:\n",
        "    o_content = o_content.replace('choices=[\"endovis\"]', 'choices=[\"endovis\", \"simcol\"]')\n",
        "if '\"simcol\"' not in o_content:\n",
        "    o_content = o_content.replace('\"hamlyn\", \"c3vd\", \"endovis\"', '\"hamlyn\", \"c3vd\", \"endovis\", \"simcol\"')\n",
        "with open(options_path, \"w\") as f:\n",
        "    f.write(o_content)\n",
        "\n",
        "# Fix error mono_dataset.py\n",
        "mono_path = os.path.join(repo_path, \"datasets\", \"mono_dataset.py\")\n",
        "with open(mono_path, \"r\") as f:\n",
        "    m_content = f.read()\n",
        "if \"sequence = folder[7]\" in m_content:\n",
        "    safe_parse = '''\n",
        "        try:\n",
        "            if \"_\" in folder and folder.split(\"_\")[-1].isdigit():\n",
        "                seq_id = int(folder.split(\"_\")[-1])\n",
        "            elif folder.isdigit():\n",
        "                seq_id = int(folder)\n",
        "            else:\n",
        "                seq_id = 0\n",
        "        except:\n",
        "            seq_id = 0\n",
        "        inputs[\"sequence\"] = torch.from_numpy(np.array(seq_id))\n",
        "        inputs[\"keyframe\"] = torch.from_numpy(np.array(0))'''\n",
        "\n",
        "    lines = m_content.splitlines()\n",
        "    new_lines = []\n",
        "    skip = 0\n",
        "    for line in lines:\n",
        "        if \"sequence = folder[7]\" in line:\n",
        "            new_lines.append(safe_parse)\n",
        "            skip = 3\n",
        "        elif skip > 0:\n",
        "            skip -= 1\n",
        "        else:\n",
        "            new_lines.append(line)\n",
        "    with open(mono_path, \"w\") as f:\n",
        "        f.write(\"\\n\".join(new_lines))\n",
        "    print(\"Fix parsing sequence at mono_dataset.py\")\n",
        "\n",
        "# SPLIT FILES\n",
        "\n",
        "split_dir = os.path.join(repo_path, \"splits\", \"simcol\")\n",
        "os.makedirs(split_dir, exist_ok=True)\n",
        "\n",
        "sequence_folders = sorted([f.name for f in os.scandir(training_data_root) if f.is_dir()])\n",
        "all_train, all_val = [], []\n",
        "\n",
        "print(f\"Split files from {len(sequence_folders)}\")\n",
        "\n",
        "for seq_name in sequence_folders:\n",
        "    seq_path = os.path.join(training_data_root, seq_name)\n",
        "    images = glob.glob(os.path.join(seq_path, \"*.png\"))\n",
        "    indices = sorted([int(os.path.basename(img).split('.')[0]) for img in images])\n",
        "\n",
        "    if len(indices) < 5: continue\n",
        "\n",
        "    valid_indices = indices[2:-2]\n",
        "    split_idx = int(len(valid_indices) * 0.9)\n",
        "\n",
        "    for idx in valid_indices[:split_idx]:\n",
        "        all_train.append(f\"{seq_name}\\t{idx}\\tl\\n\")\n",
        "    for idx in valid_indices[split_idx:]:\n",
        "        all_val.append(f\"{seq_name}\\t{idx}\\tl\\n\")\n",
        "\n",
        "with open(os.path.join(split_dir, \"train_files.txt\"), \"w\") as f: f.writelines(all_train)\n",
        "with open(os.path.join(split_dir, \"val_files.txt\"), \"w\") as f: f.writelines(all_val)\n",
        "with open(os.path.join(split_dir, \"test_files.txt\"), \"w\") as f: f.writelines(all_val)\n",
        "\n",
        "# Dummy GT depth\n",
        "dummy_depths = [np.zeros((256, 320)) for _ in range(len(all_val))]\n",
        "np.savez(os.path.join(split_dir, \"gt_depths.npz\"), data=dummy_depths)\n",
        "\n",
        "print(f\"Split files creates at: {split_dir}\")\n",
        "print(f\"Train: {len(all_train)} | Val: {len(all_val)}\")"
      ],
      "metadata": {
        "id": "cell3_config",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "83815da4-8c7d-4510-f19e-8b6a3c73d779"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… ÄÃ£ vÃ¡ lá»—i crash trong utils/layers.py (Safety Patch Applied)\n",
            "âœ… Táº¡o datasets/simcol_dataset.py (Vá»›i Normalized K chuáº©n)\n",
            "âœ… ÄÃ£ Ä‘Äƒng kÃ½ SimColDataset vÃ o Trainer\n",
            "âœ… ÄÃ£ sá»­a lá»—i parsing sequence trong mono_dataset.py\n",
            "ğŸ”„ Táº¡o split files tá»« 7 thÆ° má»¥c...\n",
            "âœ… Split files táº¡o xong táº¡i: /content/EndoDAC/splits/simcol\n",
            "ğŸ“Š Train: 7539 | Val: 840\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import sys\n",
        "\n",
        "os.environ[\"COLAB_TPU_ADDR\"] = \"\"\n",
        "\n",
        "repo_path = \"/content/EndoDAC\"\n",
        "if repo_path not in sys.path:\n",
        "    sys.path.insert(0, repo_path)\n",
        "\n",
        "model_name = \"endodac_simcol_v1\"\n",
        "log_dir = \"/content/drive/MyDrive/3D_reconstruction/outputs/EndoDAC\"\n",
        "data_path = \"/content/training_data_simcol\"\n",
        "\n",
        "os.makedirs(log_dir, exist_ok=True)\n",
        "\n",
        "!cd \"{repo_path}\" && python train_end_to_end.py \\\n",
        "  --data_path \"{data_path}\" \\\n",
        "  --model_name \"{model_name}\" \\\n",
        "  --split \"simcol\" \\\n",
        "  --eval_split \"simcol\" \\\n",
        "  --dataset \"simcol\" \\\n",
        "  --batch_size 8 \\\n",
        "  --learning_rate 1e-5 \\\n",
        "  --num_epochs 20 \\\n",
        "  --height 256 \\\n",
        "  --width 320 \\\n",
        "  --log_dir \"{log_dir}\" \\\n",
        "  --frame_ids 0 -1 1 \\\n",
        "  --min_depth 1 \\\n",
        "  --max_depth 23 \\\n",
        "  --lora_type \"dvlora\" \\\n",
        "  --lora_rank 4 \\\n",
        "  --learn_intrinsics False \\\n",
        "  --num_workers 2"
      ],
      "metadata": {
        "id": "cell4_training",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "31f38845-96b8-44a6-bd38-05895b6d63dd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/EndoDAC/models/backbones/layers/swiglu_ffn.py:43: UserWarning: xFormers is available (SwiGLU)\n",
            "  warnings.warn(\"xFormers is available (SwiGLU)\")\n",
            "/content/EndoDAC/models/backbones/layers/attention.py:27: UserWarning: xFormers is available (Attention)\n",
            "  warnings.warn(\"xFormers is available (Attention)\")\n",
            "/content/EndoDAC/models/backbones/layers/block.py:33: UserWarning: xFormers is available (Block)\n",
            "  warnings.warn(\"xFormers is available (Block)\")\n",
            "load pretrained weight from /content/EndoDAC/pretrained_model/depth_anything_vitb14.pth\n",
            "\n",
            "Training model named:\n",
            "   endodac_simcol_v1\n",
            "Models and tensorboard events files are saved to:\n",
            "   /content/drive/MyDrive/3D_reconstruction/outputs/EndoDAC\n",
            "Training is using:\n",
            "   cuda\n",
            "/usr/local/lib/python3.12/dist-packages/torch/functional.py:505: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at /pytorch/aten/src/ATen/native/TensorShape.cpp:4317.)\n",
            "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
            "Using split:\n",
            "   simcol\n",
            "There are 7539 training items, 840 validation items and 840 testing items\n",
            "\n",
            "Total params: 99091748\n",
            "Trainable params: 1667076\n",
            "Non-trainable params: 97424672\n",
            "Trainable params ratio: 1.6823560323105815%\n",
            "Training\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/functional.py:5100: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.\n",
            "  warnings.warn(\n",
            "epoch   0 | batch      0 | examples/s:   1.5 | loss: 0.10627 | time elapsed: 00h00m07s | time left: 00h00m00s\n",
            "epoch   0 | batch    400 | examples/s:   4.5 | loss: 0.05031 | time elapsed: 00h12m15s | time left: 09h25m08s\n",
            "epoch   0 | batch    800 | examples/s:   4.1 | loss: 0.03484 | time elapsed: 00h24m21s | time left: 09h09m18s\n",
            "Evaluating\n",
            "/usr/local/lib/python3.12/dist-packages/numpy/_core/fromnumeric.py:3596: RuntimeWarning: Mean of empty slice.\n",
            "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
            "/usr/local/lib/python3.12/dist-packages/numpy/_core/_methods.py:138: RuntimeWarning: invalid value encountered in scalar divide\n",
            "  ret = ret.dtype.type(ret / rcount)\n",
            "/usr/local/lib/python3.12/dist-packages/numpy/_core/_methods.py:138: RuntimeWarning: invalid value encountered in divide\n",
            "  ret = ret.dtype.type(ret / rcount)\n",
            "/content/EndoDAC/utils/utils.py:115: RuntimeWarning: Mean of empty slice.\n",
            "  a1 = (thresh < 1.25     ).mean()\n",
            "/content/EndoDAC/utils/utils.py:116: RuntimeWarning: Mean of empty slice.\n",
            "  a2 = (thresh < 1.25 ** 2).mean()\n",
            "/content/EndoDAC/utils/utils.py:117: RuntimeWarning: Mean of empty slice.\n",
            "  a3 = (thresh < 1.25 ** 3).mean()\n",
            "/content/EndoDAC/utils/utils.py:120: RuntimeWarning: Mean of empty slice.\n",
            "  rmse = np.sqrt(rmse.mean())\n",
            "/content/EndoDAC/utils/utils.py:123: RuntimeWarning: Mean of empty slice.\n",
            "  rmse_log = np.sqrt(rmse_log.mean())\n",
            " Scaling ratios | med: nan | std: nan\n",
            "NaN or Inf found in input tensor.\n",
            "NaN or Inf found in input tensor.\n",
            "NaN or Inf found in input tensor.\n",
            "NaN or Inf found in input tensor.\n",
            "NaN or Inf found in input tensor.\n",
            "NaN or Inf found in input tensor.\n",
            "NaN or Inf found in input tensor.\n",
            "\n",
            "   abs_rel |   sq_rel |     rmse | rmse_log |       a1 |       a2 |       a3 | \n",
            "&     nan  &     nan  &     nan  &     nan  &     nan  &     nan  &     nan  \\\\\n",
            "Training\n",
            "epoch   1 | batch      0 | examples/s:   4.5 | loss: 0.05065 | time elapsed: 00h31m24s | time left: 09h56m43s\n",
            "epoch   1 | batch    400 | examples/s:   4.5 | loss: 0.03885 | time elapsed: 00h43m37s | time left: 09h28m43s\n",
            "epoch   1 | batch    800 | examples/s:   4.3 | loss: 0.04196 | time elapsed: 00h55m44s | time left: 09h07m09s\n",
            "Evaluating\n",
            " Scaling ratios | med: nan | std: nan\n",
            "NaN or Inf found in input tensor.\n",
            "NaN or Inf found in input tensor.\n",
            "NaN or Inf found in input tensor.\n",
            "NaN or Inf found in input tensor.\n",
            "NaN or Inf found in input tensor.\n",
            "NaN or Inf found in input tensor.\n",
            "NaN or Inf found in input tensor.\n",
            "\n",
            "   abs_rel |   sq_rel |     rmse | rmse_log |       a1 |       a2 |       a3 | \n",
            "&     nan  &     nan  &     nan  &     nan  &     nan  &     nan  &     nan  \\\\\n",
            "Training\n",
            "epoch   2 | batch      0 | examples/s:   4.6 | loss: 0.03672 | time elapsed: 01h01m42s | time left: 09h15m24s\n",
            "epoch   2 | batch    400 | examples/s:   4.1 | loss: 0.03401 | time elapsed: 01h13m55s | time left: 08h55m50s\n",
            "epoch   2 | batch    800 | examples/s:   4.5 | loss: 0.03917 | time elapsed: 01h26m00s | time left: 08h37m40s\n",
            "Evaluating\n",
            " Scaling ratios | med: nan | std: nan\n",
            "NaN or Inf found in input tensor.\n",
            "NaN or Inf found in input tensor.\n",
            "NaN or Inf found in input tensor.\n",
            "NaN or Inf found in input tensor.\n",
            "NaN or Inf found in input tensor.\n",
            "NaN or Inf found in input tensor.\n",
            "NaN or Inf found in input tensor.\n",
            "\n",
            "   abs_rel |   sq_rel |     rmse | rmse_log |       a1 |       a2 |       a3 | \n",
            "&     nan  &     nan  &     nan  &     nan  &     nan  &     nan  &     nan  \\\\\n",
            "Training\n",
            "epoch   3 | batch      0 | examples/s:   4.4 | loss: 0.04097 | time elapsed: 01h31m56s | time left: 08h40m57s\n",
            "epoch   3 | batch    400 | examples/s:   4.4 | loss: 0.02555 | time elapsed: 01h44m10s | time left: 08h24m12s\n",
            "epoch   3 | batch    800 | examples/s:   4.5 | loss: 0.02977 | time elapsed: 01h56m17s | time left: 08h07m57s\n",
            "Evaluating\n",
            " Scaling ratios | med: nan | std: nan\n",
            "NaN or Inf found in input tensor.\n",
            "NaN or Inf found in input tensor.\n",
            "NaN or Inf found in input tensor.\n",
            "NaN or Inf found in input tensor.\n",
            "NaN or Inf found in input tensor.\n",
            "NaN or Inf found in input tensor.\n",
            "NaN or Inf found in input tensor.\n",
            "\n",
            "   abs_rel |   sq_rel |     rmse | rmse_log |       a1 |       a2 |       a3 | \n",
            "&     nan  &     nan  &     nan  &     nan  &     nan  &     nan  &     nan  \\\\\n",
            "Training\n",
            "epoch   4 | batch      0 | examples/s:   4.7 | loss: 0.03825 | time elapsed: 02h02m19s | time left: 08h09m19s\n",
            "epoch   4 | batch    400 | examples/s:   4.5 | loss: 0.04005 | time elapsed: 02h14m33s | time left: 07h53m41s\n",
            "epoch   4 | batch    800 | examples/s:   4.5 | loss: 0.02462 | time elapsed: 02h26m42s | time left: 07h38m20s\n",
            "Evaluating\n",
            " Scaling ratios | med: nan | std: nan\n",
            "NaN or Inf found in input tensor.\n",
            "NaN or Inf found in input tensor.\n",
            "NaN or Inf found in input tensor.\n",
            "NaN or Inf found in input tensor.\n",
            "NaN or Inf found in input tensor.\n",
            "NaN or Inf found in input tensor.\n",
            "NaN or Inf found in input tensor.\n",
            "\n",
            "   abs_rel |   sq_rel |     rmse | rmse_log |       a1 |       a2 |       a3 | \n",
            "&     nan  &     nan  &     nan  &     nan  &     nan  &     nan  &     nan  \\\\\n",
            "Training\n",
            "epoch   5 | batch      0 | examples/s:   4.3 | loss: 0.02575 | time elapsed: 02h32m40s | time left: 07h38m01s\n",
            "epoch   5 | batch    400 | examples/s:   4.6 | loss: 0.03567 | time elapsed: 02h44m53s | time left: 07h23m04s\n",
            "epoch   5 | batch    800 | examples/s:   4.5 | loss: 0.02432 | time elapsed: 02h57m00s | time left: 07h08m14s\n",
            "Evaluating\n",
            " Scaling ratios | med: nan | std: nan\n",
            "NaN or Inf found in input tensor.\n",
            "NaN or Inf found in input tensor.\n",
            "NaN or Inf found in input tensor.\n",
            "NaN or Inf found in input tensor.\n",
            "NaN or Inf found in input tensor.\n",
            "NaN or Inf found in input tensor.\n",
            "NaN or Inf found in input tensor.\n",
            "\n",
            "   abs_rel |   sq_rel |     rmse | rmse_log |       a1 |       a2 |       a3 | \n",
            "&     nan  &     nan  &     nan  &     nan  &     nan  &     nan  &     nan  \\\\\n",
            "Training\n",
            "epoch   6 | batch      0 | examples/s:   4.6 | loss: 0.03216 | time elapsed: 03h03m00s | time left: 07h07m02s\n",
            "epoch   6 | batch    400 | examples/s:   4.5 | loss: 0.01892 | time elapsed: 03h15m14s | time left: 06h52m33s\n",
            "epoch   6 | batch    800 | examples/s:   4.5 | loss: 0.03106 | time elapsed: 03h27m21s | time left: 06h38m07s\n",
            "Evaluating\n",
            " Scaling ratios | med: nan | std: nan\n",
            "NaN or Inf found in input tensor.\n",
            "NaN or Inf found in input tensor.\n",
            "NaN or Inf found in input tensor.\n",
            "NaN or Inf found in input tensor.\n",
            "NaN or Inf found in input tensor.\n",
            "NaN or Inf found in input tensor.\n",
            "NaN or Inf found in input tensor.\n",
            "\n",
            "   abs_rel |   sq_rel |     rmse | rmse_log |       a1 |       a2 |       a3 | \n",
            "&     nan  &     nan  &     nan  &     nan  &     nan  &     nan  &     nan  \\\\\n",
            "Training\n",
            "epoch   7 | batch      0 | examples/s:   4.5 | loss: 0.02292 | time elapsed: 03h33m17s | time left: 06h36m06s\n",
            "epoch   7 | batch    400 | examples/s:   4.5 | loss: 0.01450 | time elapsed: 03h45m29s | time left: 06h21m55s\n",
            "epoch   7 | batch    800 | examples/s:   4.0 | loss: 0.02088 | time elapsed: 03h57m35s | time left: 06h07m47s\n",
            "Evaluating\n",
            " Scaling ratios | med: nan | std: nan\n",
            "NaN or Inf found in input tensor.\n",
            "NaN or Inf found in input tensor.\n",
            "NaN or Inf found in input tensor.\n",
            "NaN or Inf found in input tensor.\n",
            "NaN or Inf found in input tensor.\n",
            "NaN or Inf found in input tensor.\n",
            "NaN or Inf found in input tensor.\n",
            "\n",
            "   abs_rel |   sq_rel |     rmse | rmse_log |       a1 |       a2 |       a3 | \n",
            "&     nan  &     nan  &     nan  &     nan  &     nan  &     nan  &     nan  \\\\\n",
            "Training\n",
            "epoch   8 | batch      0 | examples/s:   4.7 | loss: 0.02296 | time elapsed: 04h03m36s | time left: 06h05m24s\n",
            "epoch   8 | batch    400 | examples/s:   4.5 | loss: 0.02283 | time elapsed: 04h15m47s | time left: 05h51m28s\n",
            "epoch   8 | batch    800 | examples/s:   4.5 | loss: 0.02075 | time elapsed: 04h27m53s | time left: 05h37m33s\n",
            "Evaluating\n",
            " Scaling ratios | med: nan | std: nan\n",
            "NaN or Inf found in input tensor.\n",
            "NaN or Inf found in input tensor.\n",
            "NaN or Inf found in input tensor.\n",
            "NaN or Inf found in input tensor.\n",
            "NaN or Inf found in input tensor.\n",
            "NaN or Inf found in input tensor.\n",
            "NaN or Inf found in input tensor.\n",
            "\n",
            "   abs_rel |   sq_rel |     rmse | rmse_log |       a1 |       a2 |       a3 | \n",
            "&     nan  &     nan  &     nan  &     nan  &     nan  &     nan  &     nan  \\\\\n",
            "Training\n",
            "epoch   9 | batch      0 | examples/s:   4.2 | loss: 0.01538 | time elapsed: 04h33m53s | time left: 05h34m45s\n",
            "epoch   9 | batch    400 | examples/s:   4.6 | loss: 0.02066 | time elapsed: 04h46m08s | time left: 05h21m04s\n",
            "epoch   9 | batch    800 | examples/s:   4.4 | loss: 0.01939 | time elapsed: 04h58m23s | time left: 05h07m31s\n",
            "Evaluating\n",
            " Scaling ratios | med: nan | std: nan\n",
            "NaN or Inf found in input tensor.\n",
            "NaN or Inf found in input tensor.\n",
            "NaN or Inf found in input tensor.\n",
            "NaN or Inf found in input tensor.\n",
            "NaN or Inf found in input tensor.\n",
            "NaN or Inf found in input tensor.\n",
            "NaN or Inf found in input tensor.\n",
            "\n",
            "   abs_rel |   sq_rel |     rmse | rmse_log |       a1 |       a2 |       a3 | \n",
            "&     nan  &     nan  &     nan  &     nan  &     nan  &     nan  &     nan  \\\\\n",
            "Training\n",
            "epoch  10 | batch      0 | examples/s:   4.6 | loss: 0.01809 | time elapsed: 05h04m35s | time left: 05h04m35s\n",
            "epoch  10 | batch    400 | examples/s:   4.4 | loss: 0.01393 | time elapsed: 05h16m58s | time left: 04h51m09s\n",
            "epoch  10 | batch    800 | examples/s:   4.5 | loss: 0.01949 | time elapsed: 05h29m11s | time left: 04h37m39s\n",
            "Evaluating\n",
            " Scaling ratios | med: nan | std: nan\n",
            "NaN or Inf found in input tensor.\n",
            "NaN or Inf found in input tensor.\n",
            "NaN or Inf found in input tensor.\n",
            "NaN or Inf found in input tensor.\n",
            "NaN or Inf found in input tensor.\n",
            "NaN or Inf found in input tensor.\n",
            "NaN or Inf found in input tensor.\n",
            "\n",
            "   abs_rel |   sq_rel |     rmse | rmse_log |       a1 |       a2 |       a3 | \n",
            "&     nan  &     nan  &     nan  &     nan  &     nan  &     nan  &     nan  \\\\\n",
            "Training\n",
            "epoch  11 | batch      0 | examples/s:   4.6 | loss: 0.03268 | time elapsed: 05h35m27s | time left: 04h34m28s\n",
            "epoch  11 | batch    400 | examples/s:   4.5 | loss: 0.01903 | time elapsed: 05h47m58s | time left: 04h21m11s\n",
            "epoch  11 | batch    800 | examples/s:   4.5 | loss: 0.01964 | time elapsed: 06h00m12s | time left: 04h07m46s\n",
            "Evaluating\n",
            " Scaling ratios | med: nan | std: nan\n",
            "NaN or Inf found in input tensor.\n",
            "NaN or Inf found in input tensor.\n",
            "NaN or Inf found in input tensor.\n",
            "NaN or Inf found in input tensor.\n",
            "NaN or Inf found in input tensor.\n",
            "NaN or Inf found in input tensor.\n",
            "NaN or Inf found in input tensor.\n",
            "\n",
            "   abs_rel |   sq_rel |     rmse | rmse_log |       a1 |       a2 |       a3 | \n",
            "&     nan  &     nan  &     nan  &     nan  &     nan  &     nan  &     nan  \\\\\n",
            "Training\n",
            "epoch  12 | batch      0 | examples/s:   4.5 | loss: 0.01800 | time elapsed: 06h06m33s | time left: 04h04m22s\n",
            "epoch  12 | batch    400 | examples/s:   4.4 | loss: 0.01518 | time elapsed: 06h19m00s | time left: 03h51m04s\n",
            "epoch  12 | batch    800 | examples/s:   4.5 | loss: 0.01489 | time elapsed: 06h31m12s | time left: 03h37m42s\n",
            "Evaluating\n",
            " Scaling ratios | med: nan | std: nan\n",
            "NaN or Inf found in input tensor.\n",
            "NaN or Inf found in input tensor.\n",
            "NaN or Inf found in input tensor.\n",
            "NaN or Inf found in input tensor.\n",
            "NaN or Inf found in input tensor.\n",
            "NaN or Inf found in input tensor.\n",
            "NaN or Inf found in input tensor.\n",
            "\n",
            "   abs_rel |   sq_rel |     rmse | rmse_log |       a1 |       a2 |       a3 | \n",
            "&     nan  &     nan  &     nan  &     nan  &     nan  &     nan  &     nan  \\\\\n",
            "Training\n",
            "epoch  13 | batch      0 | examples/s:   4.2 | loss: 0.02688 | time elapsed: 06h37m33s | time left: 03h34m04s\n",
            "epoch  13 | batch    400 | examples/s:   4.5 | loss: 0.01723 | time elapsed: 06h49m55s | time left: 03h20m46s\n",
            "epoch  13 | batch    800 | examples/s:   4.3 | loss: 0.02090 | time elapsed: 07h02m10s | time left: 03h07m29s\n",
            "Evaluating\n",
            " Scaling ratios | med: nan | std: nan\n",
            "NaN or Inf found in input tensor.\n",
            "NaN or Inf found in input tensor.\n",
            "NaN or Inf found in input tensor.\n",
            "NaN or Inf found in input tensor.\n",
            "NaN or Inf found in input tensor.\n",
            "NaN or Inf found in input tensor.\n",
            "NaN or Inf found in input tensor.\n",
            "\n",
            "   abs_rel |   sq_rel |     rmse | rmse_log |       a1 |       a2 |       a3 | \n",
            "&     nan  &     nan  &     nan  &     nan  &     nan  &     nan  &     nan  \\\\\n",
            "Training\n",
            "epoch  14 | batch      0 | examples/s:   4.6 | loss: 0.01918 | time elapsed: 07h08m25s | time left: 03h03m36s\n",
            "epoch  14 | batch    400 | examples/s:   4.5 | loss: 0.01688 | time elapsed: 07h20m51s | time left: 02h50m23s\n",
            "epoch  14 | batch    800 | examples/s:   4.5 | loss: 0.01646 | time elapsed: 07h33m06s | time left: 02h37m10s\n",
            "Evaluating\n",
            " Scaling ratios | med: nan | std: nan\n",
            "NaN or Inf found in input tensor.\n",
            "NaN or Inf found in input tensor.\n",
            "NaN or Inf found in input tensor.\n",
            "NaN or Inf found in input tensor.\n",
            "NaN or Inf found in input tensor.\n",
            "NaN or Inf found in input tensor.\n",
            "NaN or Inf found in input tensor.\n",
            "\n",
            "   abs_rel |   sq_rel |     rmse | rmse_log |       a1 |       a2 |       a3 | \n",
            "&     nan  &     nan  &     nan  &     nan  &     nan  &     nan  &     nan  \\\\\n",
            "Training\n",
            "epoch  15 | batch      0 | examples/s:   4.5 | loss: 0.01666 | time elapsed: 07h39m26s | time left: 02h33m08s\n",
            "epoch  15 | batch    400 | examples/s:   4.5 | loss: 0.01477 | time elapsed: 07h51m56s | time left: 02h19m59s\n",
            "epoch  15 | batch    800 | examples/s:   4.3 | loss: 0.02217 | time elapsed: 08h04m15s | time left: 02h06m49s\n",
            "Evaluating\n",
            " Scaling ratios | med: nan | std: nan\n",
            "NaN or Inf found in input tensor.\n",
            "NaN or Inf found in input tensor.\n",
            "NaN or Inf found in input tensor.\n",
            "NaN or Inf found in input tensor.\n",
            "NaN or Inf found in input tensor.\n",
            "NaN or Inf found in input tensor.\n",
            "NaN or Inf found in input tensor.\n",
            "\n",
            "   abs_rel |   sq_rel |     rmse | rmse_log |       a1 |       a2 |       a3 | \n",
            "&     nan  &     nan  &     nan  &     nan  &     nan  &     nan  &     nan  \\\\\n",
            "Training\n",
            "epoch  16 | batch      0 | examples/s:   4.5 | loss: 0.01279 | time elapsed: 08h10m49s | time left: 02h02m42s\n",
            "epoch  16 | batch    400 | examples/s:   4.5 | loss: 0.01803 | time elapsed: 08h23m21s | time left: 01h49m34s\n",
            "epoch  16 | batch    800 | examples/s:   4.4 | loss: 0.02355 | time elapsed: 08h35m41s | time left: 01h36m26s\n",
            "Evaluating\n",
            " Scaling ratios | med: nan | std: nan\n",
            "NaN or Inf found in input tensor.\n",
            "NaN or Inf found in input tensor.\n",
            "NaN or Inf found in input tensor.\n",
            "NaN or Inf found in input tensor.\n",
            "NaN or Inf found in input tensor.\n",
            "NaN or Inf found in input tensor.\n",
            "NaN or Inf found in input tensor.\n",
            "\n",
            "   abs_rel |   sq_rel |     rmse | rmse_log |       a1 |       a2 |       a3 | \n",
            "&     nan  &     nan  &     nan  &     nan  &     nan  &     nan  &     nan  \\\\\n",
            "Training\n",
            "epoch  17 | batch      0 | examples/s:   4.0 | loss: 0.01598 | time elapsed: 08h42m08s | time left: 01h32m08s\n",
            "epoch  17 | batch    400 | examples/s:   4.5 | loss: 0.01669 | time elapsed: 08h54m38s | time left: 01h19m01s\n",
            "epoch  17 | batch    800 | examples/s:   4.6 | loss: 0.01390 | time elapsed: 09h06m54s | time left: 01h05m53s\n",
            "Evaluating\n",
            " Scaling ratios | med: nan | std: nan\n",
            "NaN or Inf found in input tensor.\n",
            "NaN or Inf found in input tensor.\n",
            "NaN or Inf found in input tensor.\n",
            "NaN or Inf found in input tensor.\n",
            "NaN or Inf found in input tensor.\n",
            "NaN or Inf found in input tensor.\n",
            "NaN or Inf found in input tensor.\n",
            "\n",
            "   abs_rel |   sq_rel |     rmse | rmse_log |       a1 |       a2 |       a3 | \n",
            "&     nan  &     nan  &     nan  &     nan  &     nan  &     nan  &     nan  \\\\\n",
            "Training\n",
            "epoch  18 | batch      0 | examples/s:   4.7 | loss: 0.01407 | time elapsed: 09h13m12s | time left: 01h01m28s\n",
            "epoch  18 | batch    400 | examples/s:   4.5 | loss: 0.01263 | time elapsed: 09h25m36s | time left: 00h48m21s\n",
            "epoch  18 | batch    800 | examples/s:   4.4 | loss: 0.02206 | time elapsed: 09h37m56s | time left: 00h35m16s\n",
            "Evaluating\n",
            " Scaling ratios | med: nan | std: nan\n",
            "NaN or Inf found in input tensor.\n",
            "NaN or Inf found in input tensor.\n",
            "NaN or Inf found in input tensor.\n",
            "NaN or Inf found in input tensor.\n",
            "NaN or Inf found in input tensor.\n",
            "NaN or Inf found in input tensor.\n",
            "NaN or Inf found in input tensor.\n",
            "\n",
            "   abs_rel |   sq_rel |     rmse | rmse_log |       a1 |       a2 |       a3 | \n",
            "&     nan  &     nan  &     nan  &     nan  &     nan  &     nan  &     nan  \\\\\n",
            "Training\n",
            "epoch  19 | batch      0 | examples/s:   4.7 | loss: 0.01641 | time elapsed: 09h44m22s | time left: 00h30m45s\n",
            "epoch  19 | batch    400 | examples/s:   4.2 | loss: 0.02819 | time elapsed: 09h56m56s | time left: 00h17m40s\n",
            "epoch  19 | batch    800 | examples/s:   4.3 | loss: 0.01465 | time elapsed: 10h09m11s | time left: 00h04m37s\n",
            "Evaluating\n",
            " Scaling ratios | med: nan | std: nan\n",
            "NaN or Inf found in input tensor.\n",
            "NaN or Inf found in input tensor.\n",
            "NaN or Inf found in input tensor.\n",
            "NaN or Inf found in input tensor.\n",
            "NaN or Inf found in input tensor.\n",
            "NaN or Inf found in input tensor.\n",
            "NaN or Inf found in input tensor.\n",
            "\n",
            "   abs_rel |   sq_rel |     rmse | rmse_log |       a1 |       a2 |       a3 | \n",
            "&     nan  &     nan  &     nan  &     nan  &     nan  &     nan  &     nan  \\\\\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Inference & Visualization"
      ],
      "metadata": {
        "id": "inference_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import glob\n",
        "import zipfile\n",
        "from tqdm import tqdm\n",
        "\n",
        "test_zip_path = \"/content/drive/MyDrive/3D_reconstruction/datasets/FrameBuffer_RGB.zip\"\n",
        "test_extract_path = \"/content/test_data_s1\"\n",
        "\n",
        "if not os.path.exists(test_extract_path):\n",
        "    print(f\"â³ Äang giáº£i nÃ©n S1 test data tá»« {test_zip_path}...\")\n",
        "    os.makedirs(test_extract_path, exist_ok=True)\n",
        "\n",
        "    with zipfile.ZipFile(test_zip_path, 'r') as zip_ref:\n",
        "        zip_ref.extractall(test_extract_path)\n",
        "\n",
        "    print(f\"Test data at: {test_extract_path}\")\n",
        "else:\n",
        "    print(f\"Test data has exist: {test_extract_path}\")\n",
        "\n",
        "test_images_dir = None\n",
        "for root, dirs, files in os.walk(test_extract_path):\n",
        "    if any(f.endswith('.png') for f in files):\n",
        "        test_images_dir = root\n",
        "        break\n",
        "\n",
        "if test_images_dir:\n",
        "    test_images = sorted(glob.glob(os.path.join(test_images_dir, \"*.png\")))\n",
        "    print(f\"Test images: {test_images_dir}\")\n",
        "    print(f\"frames: {len(test_images)}\")\n",
        "else:\n",
        "    print(\"No frames in file zip!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H0eyQqsLnfbw",
        "outputId": "b49cf94b-f02b-44bc-905d-339acba09843"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "â³ Äang giáº£i nÃ©n S1 test data tá»« /content/drive/MyDrive/3D_reconstruction/datasets/FrameBuffer_RGB.zip...\n",
            "âœ… Test data S1 sáºµn sÃ ng táº¡i: /content/test_data_s1\n",
            "ğŸ“‚ Test images: /content/test_data_s1/FrameBuffer_RGB\n",
            "ğŸ“Š Sá»‘ lÆ°á»£ng frames: 1201\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "repo_path = \"/content/EndoDAC\"\n",
        "model_name = \"endodac_simcol_v1\"\n",
        "log_dir = \"/content/drive/MyDrive/3D_reconstruction/outputs/EndoDAC\"\n",
        "test_images_dir = \"/content/test_data_s1/FrameBuffer_RGB\"\n",
        "output_base = \"/content/drive/MyDrive/3D_reconstruction/inference_endodac_S1\"\n",
        "\n",
        "lora_rank = 4\n",
        "lora_type = \"dvlora\"\n",
        "min_depth = 1\n",
        "max_depth = 23\n",
        "height = 256\n",
        "width = 320"
      ],
      "metadata": {
        "id": "V958ljEjmPBD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "import glob\n",
        "import os\n",
        "import sys\n",
        "\n",
        "if repo_path not in sys.path:\n",
        "    sys.path.insert(0, repo_path)\n",
        "\n",
        "import models.endodac as endodac\n",
        "import models.encoders as encoders\n",
        "import models.decoders as decoders\n",
        "from utils.layers import disp_to_depth, transformation_from_parameters\n",
        "from torchvision import transforms\n",
        "\n",
        "\n",
        "weights_folder = os.path.join(log_dir, model_name, \"models\", \"weights_last\")\n",
        "if not os.path.exists(weights_folder):\n",
        "    weights_folder = os.path.join(log_dir, model_name, \"models\", \"weights_epoch\")\n",
        "print(f\"Loading weights: {weights_folder}\")\n",
        "\n",
        "# Load Depth Model\n",
        "print(\"Loading Depth Model\")\n",
        "depth_model = endodac.endodac(\n",
        "    backbone_size=\"base\",\n",
        "    r=lora_rank,\n",
        "    lora_type=lora_type,\n",
        "    image_shape=(224, 280),\n",
        "    pretrained_path=os.path.join(repo_path, \"pretrained_model\"),\n",
        "    residual_block_indexes=[2, 5, 8, 11],\n",
        "    include_cls_token=True\n",
        ")\n",
        "\n",
        "depth_weights_path = os.path.join(weights_folder, \"depth_model.pth\")\n",
        "depth_weights = torch.load(depth_weights_path, map_location=device)\n",
        "model_dict = depth_model.state_dict()\n",
        "depth_model.load_state_dict({k: v for k, v in depth_weights.items() if k in model_dict})\n",
        "depth_model.to(device)\n",
        "depth_model.eval()\n",
        "print(\"Depth Model loaded!\")\n",
        "\n",
        "# Load Pose Model\n",
        "print(\"Loading Pose Model\")\n",
        "pose_encoder = encoders.ResnetEncoder(18, False, num_input_images=2)\n",
        "pose_decoder = decoders.PoseDecoder(pose_encoder.num_ch_enc, num_input_features=1, num_frames_to_predict_for=2)\n",
        "\n",
        "pose_enc_path = os.path.join(weights_folder, \"pose_encoder.pth\")\n",
        "pose_dec_path = os.path.join(weights_folder, \"pose.pth\")\n",
        "\n",
        "if os.path.exists(pose_enc_path) and os.path.exists(pose_dec_path):\n",
        "    pose_encoder.load_state_dict(torch.load(pose_enc_path, map_location=device))\n",
        "    pose_decoder.load_state_dict(torch.load(pose_dec_path, map_location=device))\n",
        "    pose_encoder.to(device)\n",
        "    pose_decoder.to(device)\n",
        "    pose_encoder.eval()\n",
        "    pose_decoder.eval()\n",
        "    print(\"Pose Model loaded!\")\n",
        "else:\n",
        "    print(\"No Pose model \")\n",
        "    pose_encoder = None\n",
        "    pose_decoder = None"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lO5Rjz6kmPZT",
        "outputId": "9f3e9040-e5c5-46b8-f25b-f344fa17d19f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/content/EndoDAC/models/backbones/layers/swiglu_ffn.py:43: UserWarning: xFormers is available (SwiGLU)\n",
            "  warnings.warn(\"xFormers is available (SwiGLU)\")\n",
            "/content/EndoDAC/models/backbones/layers/attention.py:27: UserWarning: xFormers is available (Attention)\n",
            "  warnings.warn(\"xFormers is available (Attention)\")\n",
            "/content/EndoDAC/models/backbones/layers/block.py:33: UserWarning: xFormers is available (Block)\n",
            "  warnings.warn(\"xFormers is available (Block)\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ–¥ï¸ Device: cuda\n",
            "ğŸ“‚ Loading weights tá»«: /content/drive/MyDrive/3D_reconstruction/outputs/EndoDAC/endodac_simcol_v1/models/weights_last\n",
            "â³ Loading Depth Model...\n",
            "load pretrained weight from /content/EndoDAC/pretrained_model/depth_anything_vitb14.pth\n",
            "\n",
            "âœ… Depth Model loaded!\n",
            "â³ Loading Pose Model...\n",
            "âœ… Pose Model loaded!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "depth_output_dir = os.path.join(output_base, \"depth\")\n",
        "os.makedirs(depth_output_dir, exist_ok=True)\n",
        "\n",
        "test_images = sorted(glob.glob(os.path.join(test_images_dir, \"*.png\")))\n",
        "print(f\"Processing {len(test_images)} frames...\")\n",
        "\n",
        "to_tensor = transforms.ToTensor()\n",
        "poses = [np.eye(4)]\n",
        "current_pose = np.eye(4)\n",
        "\n",
        "with torch.no_grad():\n",
        "    for i in tqdm(range(len(test_images))):\n",
        "        # Load and preprocess image\n",
        "        img_pil = Image.open(test_images[i]).convert('RGB')\n",
        "        img_resized = img_pil.resize((width, height), Image.LANCZOS)\n",
        "        img_tensor = to_tensor(img_resized).unsqueeze(0).to(device)\n",
        "\n",
        "        # DEPTH PREDICTION\n",
        "        outputs = depth_model(img_tensor)\n",
        "        disp = outputs[(\"disp\", 0)]\n",
        "        disp_np = disp[0, 0].cpu().numpy()\n",
        "        # Convert Disparity to Depth\n",
        "        depth_np = 1.0 / np.maximum(disp_np, 1e-7)\n",
        "        # .npy\n",
        "        np.save(os.path.join(depth_output_dir, f\"{i:010d}.npy\"), depth_np)\n",
        "        # VISUALIZATION DEPTH\n",
        "        #plt.imsave(\n",
        "            #os.path.join(depth_output_dir, f\"{i:010d}.png\"),\n",
        "            #depth_np,\n",
        "            #cmap='plasma'\n",
        "        #)\n",
        "\n",
        "        # LÆ°u .npy (giÃ¡ trá»‹ thÃ´)\n",
        "        np.save(os.path.join(depth_output_dir, f\"{i:010d}.npy\"), disp_np)\n",
        "        # VISUALIZATION DEPTH\n",
        "        plt.imsave(\n",
        "            os.path.join(depth_output_dir, f\"{i:010d}.png\"),\n",
        "            disp_np,\n",
        "            cmap='magma'\n",
        "        )\n",
        "\n",
        "        # POSE PREDICTION\n",
        "        if pose_encoder is not None and i < len(test_images) - 1:\n",
        "            img_next_pil = Image.open(test_images[i + 1]).convert('RGB')\n",
        "            img_next_resized = img_next_pil.resize((width, height), Image.LANCZOS)\n",
        "            img_next_tensor = to_tensor(img_next_resized).unsqueeze(0).to(device)\n",
        "\n",
        "            pose_input = torch.cat([img_next_tensor, img_tensor], dim=1)\n",
        "            features = [pose_encoder(pose_input)]\n",
        "            axisangle, translation, _ = pose_decoder(features)\n",
        "\n",
        "            pose_mat = transformation_from_parameters(\n",
        "                axisangle[:, 0], translation[:, 0]\n",
        "            ).cpu().numpy()[0]\n",
        "\n",
        "            current_pose = current_pose @ pose_mat\n",
        "            poses.append(current_pose.copy())\n",
        "\n",
        "print(f\"\\n Depth maps saved to: {depth_output_dir}\")\n",
        "print(f\"Total frames: {len(test_images)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lGEjnqvtmRWS",
        "outputId": "778088aa-9829-497c-ea04-ae963b324816"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ“Š Processing 1201 frames...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 14%|â–ˆâ–        | 164/1201 [02:40<14:16,  1.21it/s]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# SAVE TRAJECTORY\n",
        "trajectory_path = os.path.join(output_base, \"trajectory_S1.txt\")\n",
        "with open(trajectory_path, \"w\") as f:\n",
        "    for pose in poses:\n",
        "        row_str = \" \".join(map(str, pose[:3, :].flatten()))\n",
        "        f.write(row_str + \"\\n\")\n",
        "\n",
        "npz_path = os.path.join(output_base, \"poses_S1.npz\")\n",
        "np.savez_compressed(npz_path, poses=np.array(poses))\n",
        "\n",
        "print(f\" Trajectory saved to: {trajectory_path}\")\n",
        "print(f\" Poses saved to: {npz_path}\")\n",
        "print(f\" Total poses: {len(poses)}\")"
      ],
      "metadata": {
        "id": "rCFU5fmLmTzd"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}