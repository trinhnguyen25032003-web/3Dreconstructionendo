{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "setup_header"
      },
      "source": [
        "## 1. Setup Environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cell1_setup",
        "outputId": "d07ad8a3-1950-4bc2-a702-ba569788a491"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m87.2/87.2 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCloning into 'AF-SfMLearner'...\n",
            "remote: Enumerating objects: 254, done.\u001b[K\n",
            "remote: Counting objects: 100% (71/71), done.\u001b[K\n",
            "remote: Compressing objects: 100% (39/39), done.\u001b[K\n",
            "remote: Total 254 (delta 40), reused 60 (delta 32), pack-reused 183 (from 1)\u001b[K\n",
            "Receiving objects: 100% (254/254), 4.76 MiB | 34.31 MiB/s, done.\n",
            "Resolving deltas: 100% (114/114), done.\n",
            "‚úÖ Environment setup ho√†n t·∫•t!\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "import sys\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "!pip install -q tensorboardX\n",
        "!git clone https://github.com/ShuweiShao/AF-SfMLearner\n",
        "repo_path = \"/content/AF-SfMLearner\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "data_header"
      },
      "source": [
        "Data prepare"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cell2_data",
        "outputId": "286db878-7316-4910-b20a-73897b978f2e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚è≥ ƒêang gi·∫£i n√©n d·ªØ li·ªáu t·ª´ /content/drive/MyDrive/3D_reconstruction/datasets/FrameBufferS2S8.zip...\n",
            "üìÇ Sequences: ['FrameBuffer_S5', 'FrameBuffer_S8', 'FrameBuffer_S6', 'FrameBuffer_S4', 'FrameBuffer_S7', 'FrameBuffer_S2', 'FrameBuffer_S3']\n",
            "‚è≥ ƒêang chu·∫©n h√≥a t√™n file...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Rename FrameBuffer_S5: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1201/1201 [00:00<00:00, 51458.33it/s]\n",
            "Rename FrameBuffer_S8: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1201/1201 [00:00<00:00, 47349.83it/s]\n",
            "Rename FrameBuffer_S6: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1201/1201 [00:00<00:00, 45420.08it/s]\n",
            "Rename FrameBuffer_S4: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1201/1201 [00:00<00:00, 45841.68it/s]\n",
            "Rename FrameBuffer_S7: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1201/1201 [00:00<00:00, 44392.19it/s]\n",
            "Rename FrameBuffer_S2: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1201/1201 [00:00<00:00, 47075.04it/s]\n",
            "Rename FrameBuffer_S3: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1201/1201 [00:00<00:00, 49259.83it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ D·ªØ li·ªáu s·∫µn s√†ng t·∫°i: /content/training_data_simcol\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import glob\n",
        "import zipfile\n",
        "from tqdm import tqdm\n",
        "\n",
        "train_zip_path = \"/content/drive/MyDrive/3D_reconstruction/datasets/FrameBufferS2S8.zip\"\n",
        "training_data_root = \"/content/training_data_simcol\"\n",
        "\n",
        "if not os.path.exists(training_data_root):\n",
        "    print(f\" Extracting: {train_zip_path}...\")\n",
        "    os.makedirs(training_data_root, exist_ok=True)\n",
        "\n",
        "    with zipfile.ZipFile(train_zip_path, 'r') as zip_ref:\n",
        "        zip_ref.extractall(training_data_root)\n",
        "\n",
        "    # Get sequences lists\n",
        "    subfolders = [f.path for f in os.scandir(training_data_root) if f.is_dir()]\n",
        "    print(f\"Sequences: {[os.path.basename(f) for f in subfolders]}\")\n",
        "\n",
        "    # Correct file name: FrameBuffer_0000.png -> 0000000000.png\n",
        "    print(\"Correct file name\")\n",
        "    for folder in subfolders:\n",
        "        images = glob.glob(os.path.join(folder, \"*.png\"))\n",
        "        for img_path in tqdm(images, desc=f\"Rename {os.path.basename(folder)}\"):\n",
        "            folder_path, filename = os.path.split(img_path)\n",
        "            if \"FrameBuffer_\" in filename:\n",
        "                new_name = filename.replace(\"FrameBuffer_\", \"\")\n",
        "                try:\n",
        "                    num = int(new_name.split('.')[0])\n",
        "                    new_filename = \"{:010d}.png\".format(num)\n",
        "                    os.rename(img_path, os.path.join(folder_path, new_filename))\n",
        "                except ValueError:\n",
        "                    pass\n",
        "\n",
        "    print(f\"Data ready at: {training_data_root}\")\n",
        "else:\n",
        "    subfolders = [f.path for f in os.scandir(training_data_root) if f.is_dir()]\n",
        "    print(f\"Data has existed: {training_data_root}\")\n",
        "    print(f\"Sequences: {[os.path.basename(f) for f in subfolders]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "split_header"
      },
      "source": [
        "Split Files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cell3_splits",
        "outputId": "c3a0140e-600f-4ed6-da35-33a81e2f5d20"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üîÑ T·∫°o split t·ª´ sequences: ['FrameBuffer_S2', 'FrameBuffer_S3', 'FrameBuffer_S4', 'FrameBuffer_S5', 'FrameBuffer_S6', 'FrameBuffer_S7', 'FrameBuffer_S8']\n",
            "   üìÅ FrameBuffer_S2: 1079 train, 120 val\n",
            "   üìÅ FrameBuffer_S3: 1079 train, 120 val\n",
            "   üìÅ FrameBuffer_S4: 1079 train, 120 val\n",
            "   üìÅ FrameBuffer_S5: 1079 train, 120 val\n",
            "   üìÅ FrameBuffer_S6: 1079 train, 120 val\n",
            "   üìÅ FrameBuffer_S7: 1079 train, 120 val\n",
            "   üìÅ FrameBuffer_S8: 1079 train, 120 val\n",
            "\n",
            "‚úÖ Split files t·∫°i: /content/AF-SfMLearner/splits/simcol_sequences\n",
            "üìä T·ªïng Train: 7553 samples\n",
            "üìä T·ªïng Val: 840 samples\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import glob\n",
        "\n",
        "repo_path = \"/content/AF-SfMLearner\"\n",
        "training_data_root = \"/content/training_data_simcol\"\n",
        "split_name = \"simcol_sequences\"\n",
        "split_dir = os.path.join(repo_path, \"splits\", split_name)\n",
        "\n",
        "os.makedirs(split_dir, exist_ok=True)\n",
        "\n",
        "sequence_folders = sorted([f.name for f in os.scandir(training_data_root) if f.is_dir()])\n",
        "print(f\" Create split for sequences: {sequence_folders}\")\n",
        "\n",
        "all_train_lines = []\n",
        "all_val_lines = []\n",
        "\n",
        "for seq_name in sequence_folders:\n",
        "    seq_path = os.path.join(training_data_root, seq_name)\n",
        "    images = glob.glob(os.path.join(seq_path, \"*.png\"))\n",
        "    indices = sorted([int(os.path.basename(img).split('.')[0]) for img in images])\n",
        "\n",
        "    if len(indices) < 3:\n",
        "        continue\n",
        "\n",
        "    # Get rid of first and last frame\n",
        "    valid_indices = indices[1:-1]\n",
        "\n",
        "    # 90% train, 10% val\n",
        "    split_idx = int(len(valid_indices) * 0.9)\n",
        "    train_indices = valid_indices[:split_idx]\n",
        "    val_indices = valid_indices[split_idx:]\n",
        "\n",
        "    # Format: \"folder  frame_index  side\"\n",
        "    for idx in train_indices:\n",
        "        all_train_lines.append(f\"{seq_name} {idx} l\\n\")\n",
        "    for idx in val_indices:\n",
        "        all_val_lines.append(f\"{seq_name} {idx} l\\n\")\n",
        "\n",
        "    print(f\"   üìÅ {seq_name}: {len(train_indices)} train, {len(val_indices)} val\")\n",
        "\n",
        "# Ghi files\n",
        "with open(os.path.join(split_dir, \"train_files.txt\"), \"w\") as f:\n",
        "    f.writelines(all_train_lines)\n",
        "with open(os.path.join(split_dir, \"val_files.txt\"), \"w\") as f:\n",
        "    f.writelines(all_val_lines)\n",
        "\n",
        "print(f\"\\n Split files at: {split_dir}\")\n",
        "print(f\" Train: {len(all_train_lines)} samples\")\n",
        "print(f\" Val: {len(all_val_lines)} samples\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "patch_header"
      },
      "source": [
        "Patch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cell4_patch",
        "outputId": "b176f0cb-c0e8-4072-cc5c-43856ae6d61c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Ghi file: simcol_dataset.py\n",
            "‚úÖ Ghi file: resnet_encoder.py\n",
            "‚úÖ Th√™m SimColDataset v√†o __init__.py\n",
            "‚úÖ Patch: trainer_end_to_end.py - thay 'datasets_dict = {\"endovis\": da...'\n",
            "‚úÖ Patch: options.py - thay 'choices=[\"endovis\", \"eigen_zho...'\n",
            "‚úÖ Patch: options.py - thay 'choices=[\"endovis\", \"kitti\", \"...'\n",
            "‚úÖ Patch: mono_dataset.py - thay 'color_aug = transforms.ColorJi...'\n",
            "‚úÖ Patch: mono_dataset.py - thay 'Image.ANTIALIAS...'\n",
            "\n",
            "üéâ To√†n b·ªô patches ƒë√£ ho√†n t·∫•t!\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "\n",
        "repo_path = \"/content/AF-SfMLearner\"\n",
        "\n",
        "def overwrite_file(path, content):\n",
        "    with open(path, \"w\") as f:\n",
        "        f.write(content)\n",
        "    print(f\"Write file: {os.path.basename(path)}\")\n",
        "\n",
        "def replace_in_file(path, old, new):\n",
        "    if os.path.exists(path):\n",
        "        with open(path, \"r\") as f:\n",
        "            content = f.read()\n",
        "        if old in content and new not in content:\n",
        "            with open(path, \"w\") as f:\n",
        "                f.write(content.replace(old, new))\n",
        "            print(f\"Patch: {os.path.basename(path)} - replace '{old[:30]}...'\")\n",
        "            return True\n",
        "    return False\n",
        "\n",
        "# Create SimColDataset\n",
        "\n",
        "simcol_dataset_content = '''from __future__ import absolute_import, division, print_function\n",
        "import os\n",
        "import numpy as np\n",
        "import PIL.Image as pil\n",
        "from .mono_dataset import MonoDataset\n",
        "\n",
        "class SimColDataset(MonoDataset):\n",
        "    \"\"\"SimCol3D Dataset - Load theo sequences\n",
        "    Format file: folder  frame_index  side\n",
        "    \"\"\"\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super(SimColDataset, self).__init__(*args, **kwargs)\n",
        "\n",
        "        # SimCol camera intrinsics (normalized)\n",
        "        H, W = 475, 475\n",
        "        fx_norm = 227.60416 / W\n",
        "        fy_norm = 237.5 / H\n",
        "        cx_norm = 227.604416 / W\n",
        "        cy_norm = 237.5 / H\n",
        "\n",
        "        self.K = np.array([[fx_norm, 0., cx_norm, 0.],\n",
        "                           [0., fy_norm, cy_norm, 0.],\n",
        "                           [0., 0., 1., 0.],\n",
        "                           [0., 0., 0., 1.]], dtype=np.float32)\n",
        "\n",
        "    def check_depth(self):\n",
        "        return False\n",
        "\n",
        "    def get_color(self, folder, frame_index, side, do_flip):\n",
        "        # folder = sequence name (e.g., \"FrameBuffer_S2\")\n",
        "        # frame_index = frame number within that sequence\n",
        "        f_str = \"{:010d}.png\".format(frame_index)\n",
        "        image_path = os.path.join(self.data_path, folder, f_str)\n",
        "\n",
        "        with open(image_path, 'rb') as f:\n",
        "            with pil.open(f) as img:\n",
        "                color = img.convert('RGB')\n",
        "\n",
        "        if do_flip:\n",
        "            color = color.transpose(pil.FLIP_LEFT_RIGHT)\n",
        "        return color\n",
        "'''\n",
        "\n",
        "overwrite_file(os.path.join(repo_path, \"datasets\", \"simcol_dataset.py\"), simcol_dataset_content)\n",
        "\n",
        "# Fix ResnetEncoder\n",
        "\n",
        "resnet_content = '''from __future__ import absolute_import, division, print_function\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.models as models\n",
        "\n",
        "class ResNetMultiImageInput(models.ResNet):\n",
        "    def __init__(self, block, layers, num_input_images=1):\n",
        "        super(ResNetMultiImageInput, self).__init__(block, layers)\n",
        "        self.inplanes = 64\n",
        "        self.conv1 = nn.Conv2d(num_input_images * 3, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "        self.layer1 = self._make_layer(block, 64, layers[0])\n",
        "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n",
        "        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n",
        "        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                nn.init.constant_(m.weight, 1)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "\n",
        "def resnet_multiimage_input(num_layers, pretrained=False, num_input_images=1):\n",
        "    blocks = {18: [2, 2, 2, 2], 50: [3, 4, 6, 3]}[num_layers]\n",
        "    block_type = {18: models.resnet.BasicBlock, 50: models.resnet.Bottleneck}[num_layers]\n",
        "    model = ResNetMultiImageInput(block_type, blocks, num_input_images=num_input_images)\n",
        "    if pretrained:\n",
        "        loaded = models.resnet18(weights=\"IMAGENET1K_V1\").state_dict() if num_layers == 18 else models.resnet50(weights=\"IMAGENET1K_V1\").state_dict()\n",
        "        loaded['conv1.weight'] = torch.cat([loaded['conv1.weight']] * num_input_images, 1) / num_input_images\n",
        "        model.load_state_dict(loaded)\n",
        "    return model\n",
        "\n",
        "class ResnetEncoder(nn.Module):\n",
        "    def __init__(self, num_layers, pretrained, num_input_images=1):\n",
        "        super(ResnetEncoder, self).__init__()\n",
        "        self.num_ch_enc = np.array([64, 64, 128, 256, 512])\n",
        "        if num_input_images > 1:\n",
        "            self.encoder = resnet_multiimage_input(num_layers, pretrained, num_input_images)\n",
        "        else:\n",
        "            weights = \"IMAGENET1K_V1\" if pretrained else None\n",
        "            resnets = {18: models.resnet18, 34: models.resnet34, 50: models.resnet50}\n",
        "            self.encoder = resnets[num_layers](weights=weights)\n",
        "        if num_layers > 34:\n",
        "            self.num_ch_enc[1:] *= 4\n",
        "\n",
        "    def forward(self, input_image):\n",
        "        self.features = []\n",
        "        x = self.encoder.conv1(input_image)\n",
        "        x = self.encoder.bn1(x)\n",
        "        self.features.append(self.encoder.relu(x))\n",
        "        self.features.append(self.encoder.layer1(self.encoder.maxpool(self.features[-1])))\n",
        "        self.features.append(self.encoder.layer2(self.features[-1]))\n",
        "        self.features.append(self.encoder.layer3(self.features[-1]))\n",
        "        self.features.append(self.encoder.layer4(self.features[-1]))\n",
        "        return self.features\n",
        "'''\n",
        "\n",
        "overwrite_file(os.path.join(repo_path, \"networks\", \"resnet_encoder.py\"), resnet_content)\n",
        "\n",
        "# Update __init__.py\n",
        "\n",
        "init_path = os.path.join(repo_path, \"datasets\", \"__init__.py\")\n",
        "with open(init_path, \"r\") as f:\n",
        "    content = f.read()\n",
        "if \"SimColDataset\" not in content:\n",
        "    with open(init_path, \"a\") as f:\n",
        "        f.write(\"\\nfrom .simcol_dataset import SimColDataset\\n\")\n",
        "    print(\" Create SimColDataset in __init__.py\")\n",
        "\n",
        "# Update trainer_end_to_end.py - Add simcol into datasets_dict\n",
        "\n",
        "trainer_path = os.path.join(repo_path, \"trainer_end_to_end.py\")\n",
        "replace_in_file(trainer_path,\n",
        "    'datasets_dict = {\"endovis\": datasets.SCAREDRAWDataset}',\n",
        "    'datasets_dict = {\"endovis\": datasets.SCAREDRAWDataset, \"simcol\": datasets.SimColDataset}')\n",
        "\n",
        "# Update options.py - Add choices for split and dataset\n",
        "\n",
        "options_path = os.path.join(repo_path, \"options.py\")\n",
        "\n",
        "# Add simcol_sequences to split choices\n",
        "replace_in_file(options_path,\n",
        "    'choices=[\"endovis\", \"eigen_zhou\", \"eigen_full\", \"odom\", \"benchmark\"]',\n",
        "    'choices=[\"endovis\", \"eigen_zhou\", \"eigen_full\", \"odom\", \"benchmark\", \"simcol_sequences\"]')\n",
        "\n",
        "# Add simcol to dataset choices\n",
        "replace_in_file(options_path,\n",
        "    'choices=[\"endovis\", \"kitti\", \"kitti_odom\", \"kitti_depth\", \"kitti_test\"]',\n",
        "    'choices=[\"endovis\", \"kitti\", \"kitti_odom\", \"kitti_depth\", \"kitti_test\", \"simcol\"]')\n",
        "\n",
        "\n",
        "# Fix deprecated functions in mono_dataset.py\n",
        "\n",
        "mono_path = os.path.join(repo_path, \"datasets\", \"mono_dataset.py\")\n",
        "replace_in_file(mono_path,\n",
        "    \"color_aug = transforms.ColorJitter.get_params(\\n                self.brightness, self.contrast, self.saturation, self.hue)\",\n",
        "    \"color_aug = transforms.ColorJitter(self.brightness, self.contrast, self.saturation, self.hue)\")\n",
        "replace_in_file(mono_path, \"Image.ANTIALIAS\", \"Image.LANCZOS\")\n",
        "\n",
        "print(\"\\n Finished patch!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "training_header"
      },
      "source": [
        "## 5. Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cell5_training",
        "outputId": "c44e994e-93f1-408d-e99c-7f2b0293efad"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üöÄ Training: simcol_sequences_model_320\n",
            "üìä Data: Sequences (kh√¥ng g·ªôp)\n",
            "üìä Epochs: 30\n",
            "üìä Resolution: 320x320\n",
            "\n",
            "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n",
            "100% 44.7M/44.7M [00:00<00:00, 164MB/s]\n",
            "Training model named:\n",
            "   simcol_sequences_model_320\n",
            "Models and tensorboard events files are saved to:\n",
            "   /content/drive/MyDrive/3D_reconstruction/outputs/AF-SfMLearner\n",
            "Training is using:\n",
            "   cuda\n",
            "/usr/local/lib/python3.12/dist-packages/torch/functional.py:505: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at /pytorch/aten/src/ATen/native/TensorShape.cpp:4317.)\n",
            "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
            "Using split:\n",
            "   simcol_sequences\n",
            "There are 7553 training items and 840 validation items\n",
            "\n",
            "Training\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/functional.py:5100: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.\n",
            "  warnings.warn(\n",
            "epoch   0 | batch      0 | examples/s:   1.7 | loss: 0.09742 | time elapsed: 00h00m09s | time left: 00h00m00s\n",
            "epoch   0 | batch    200 | examples/s:   4.9 | loss: 0.03429 | time elapsed: 00h05m29s | time left: 10h42m57s\n",
            "epoch   0 | batch    400 | examples/s:   5.3 | loss: 0.03010 | time elapsed: 00h10m53s | time left: 10h32m00s\n",
            "epoch   0 | batch    600 | examples/s:   5.0 | loss: 0.03225 | time elapsed: 00h16m15s | time left: 10h23m06s\n",
            "epoch   0 | batch    800 | examples/s:   5.3 | loss: 0.02063 | time elapsed: 00h21m37s | time left: 10h16m26s\n",
            "Training\n",
            "epoch   1 | batch      0 | examples/s:   5.8 | loss: 0.02668 | time elapsed: 00h26m02s | time left: 10h24m48s\n",
            "epoch   1 | batch    200 | examples/s:   5.2 | loss: 0.02068 | time elapsed: 00h31m31s | time left: 10h18m53s\n",
            "epoch   1 | batch    400 | examples/s:   5.3 | loss: 0.02578 | time elapsed: 00h36m54s | time left: 10h11m15s\n",
            "epoch   1 | batch    600 | examples/s:   5.2 | loss: 0.01862 | time elapsed: 00h42m16s | time left: 10h03m52s\n",
            "epoch   1 | batch    800 | examples/s:   5.2 | loss: 0.02463 | time elapsed: 00h47m36s | time left: 09h56m39s\n",
            "Training\n",
            "epoch   2 | batch      0 | examples/s:   5.4 | loss: 0.01954 | time elapsed: 00h52m02s | time left: 09h58m32s\n",
            "epoch   2 | batch    200 | examples/s:   5.3 | loss: 0.01445 | time elapsed: 00h57m31s | time left: 09h52m37s\n",
            "epoch   2 | batch    400 | examples/s:   5.0 | loss: 0.01538 | time elapsed: 01h02m52s | time left: 09h45m36s\n",
            "epoch   2 | batch    600 | examples/s:   5.0 | loss: 0.01610 | time elapsed: 01h08m12s | time left: 09h38m48s\n",
            "epoch   2 | batch    800 | examples/s:   5.4 | loss: 0.01775 | time elapsed: 01h13m32s | time left: 09h32m04s\n",
            "Training\n",
            "epoch   3 | batch      0 | examples/s:   5.3 | loss: 0.02075 | time elapsed: 01h17m56s | time left: 09h31m36s\n",
            "epoch   3 | batch    200 | examples/s:   5.1 | loss: 0.01617 | time elapsed: 01h23m26s | time left: 09h25m59s\n",
            "epoch   3 | batch    400 | examples/s:   5.3 | loss: 0.01656 | time elapsed: 01h28m44s | time left: 09h19m16s\n",
            "epoch   3 | batch    600 | examples/s:   4.4 | loss: 0.01167 | time elapsed: 01h34m05s | time left: 09h12m55s\n",
            "epoch   3 | batch    800 | examples/s:   5.2 | loss: 0.02052 | time elapsed: 01h39m30s | time left: 09h07m03s\n",
            "Training\n",
            "epoch   4 | batch      0 | examples/s:   5.2 | loss: 0.01225 | time elapsed: 01h43m57s | time left: 09h05m44s\n",
            "epoch   4 | batch    200 | examples/s:   5.0 | loss: 0.01785 | time elapsed: 01h49m28s | time left: 09h00m18s\n",
            "epoch   4 | batch    400 | examples/s:   5.3 | loss: 0.01373 | time elapsed: 01h54m50s | time left: 08h54m11s\n",
            "epoch   4 | batch    600 | examples/s:   5.1 | loss: 0.01465 | time elapsed: 02h00m11s | time left: 08h48m02s\n",
            "epoch   4 | batch    800 | examples/s:   5.3 | loss: 0.01913 | time elapsed: 02h05m35s | time left: 08h42m05s\n",
            "Training\n",
            "epoch   5 | batch      0 | examples/s:   5.6 | loss: 0.01193 | time elapsed: 02h10m02s | time left: 08h40m10s\n",
            "epoch   5 | batch    200 | examples/s:   5.2 | loss: 0.01503 | time elapsed: 02h15m33s | time left: 08h34m42s\n",
            "epoch   5 | batch    400 | examples/s:   5.2 | loss: 0.01292 | time elapsed: 02h20m56s | time left: 08h28m42s\n",
            "epoch   5 | batch    600 | examples/s:   4.9 | loss: 0.01255 | time elapsed: 02h26m19s | time left: 08h22m47s\n",
            "epoch   5 | batch    800 | examples/s:   5.0 | loss: 0.01124 | time elapsed: 02h31m41s | time left: 08h16m51s\n",
            "Training\n",
            "epoch   6 | batch      0 | examples/s:   5.6 | loss: 0.00892 | time elapsed: 02h36m06s | time left: 08h14m21s\n",
            "epoch   6 | batch    200 | examples/s:   5.1 | loss: 0.01364 | time elapsed: 02h41m38s | time left: 08h08m52s\n",
            "epoch   6 | batch    400 | examples/s:   5.4 | loss: 0.01407 | time elapsed: 02h46m59s | time left: 08h02m54s\n",
            "epoch   6 | batch    600 | examples/s:   5.1 | loss: 0.01247 | time elapsed: 02h52m20s | time left: 07h56m57s\n",
            "epoch   6 | batch    800 | examples/s:   5.1 | loss: 0.01187 | time elapsed: 02h57m40s | time left: 07h51m00s\n",
            "Training\n",
            "epoch   7 | batch      0 | examples/s:   5.5 | loss: 0.01025 | time elapsed: 03h02m04s | time left: 07h48m11s\n",
            "epoch   7 | batch    200 | examples/s:   4.8 | loss: 0.00690 | time elapsed: 03h07m27s | time left: 07h42m21s\n",
            "epoch   7 | batch    400 | examples/s:   5.0 | loss: 0.01044 | time elapsed: 03h12m46s | time left: 07h36m24s\n",
            "epoch   7 | batch    600 | examples/s:   5.3 | loss: 0.01279 | time elapsed: 03h18m06s | time left: 07h30m31s\n",
            "epoch   7 | batch    800 | examples/s:   5.1 | loss: 0.01098 | time elapsed: 03h23m24s | time left: 07h24m36s\n",
            "Training\n",
            "epoch   8 | batch      0 | examples/s:   5.3 | loss: 0.01653 | time elapsed: 03h27m50s | time left: 07h21m38s\n",
            "epoch   8 | batch    200 | examples/s:   5.1 | loss: 0.01000 | time elapsed: 03h33m18s | time left: 07h16m05s\n",
            "epoch   8 | batch    400 | examples/s:   5.2 | loss: 0.01412 | time elapsed: 03h38m40s | time left: 07h10m18s\n",
            "epoch   8 | batch    600 | examples/s:   5.2 | loss: 0.01555 | time elapsed: 03h44m03s | time left: 07h04m35s\n",
            "epoch   8 | batch    800 | examples/s:   5.2 | loss: 0.01087 | time elapsed: 03h49m26s | time left: 06h58m52s\n",
            "Training\n",
            "epoch   9 | batch      0 | examples/s:   5.7 | loss: 0.00864 | time elapsed: 03h53m53s | time left: 06h55m48s\n",
            "epoch   9 | batch    200 | examples/s:   4.6 | loss: 0.00845 | time elapsed: 03h59m21s | time left: 06h50m14s\n",
            "epoch   9 | batch    400 | examples/s:   5.2 | loss: 0.01106 | time elapsed: 04h04m40s | time left: 06h44m25s\n",
            "epoch   9 | batch    600 | examples/s:   5.2 | loss: 0.00872 | time elapsed: 04h10m01s | time left: 06h38m41s\n",
            "epoch   9 | batch    800 | examples/s:   5.1 | loss: 0.01466 | time elapsed: 04h15m23s | time left: 06h32m58s\n",
            "Training\n",
            "epoch  10 | batch      0 | examples/s:   4.9 | loss: 0.00892 | time elapsed: 04h19m55s | time left: 06h29m53s\n",
            "epoch  10 | batch    200 | examples/s:   5.2 | loss: 0.00829 | time elapsed: 04h25m24s | time left: 06h24m21s\n",
            "epoch  10 | batch    400 | examples/s:   5.2 | loss: 0.01396 | time elapsed: 04h30m45s | time left: 06h18m37s\n",
            "epoch  10 | batch    600 | examples/s:   4.9 | loss: 0.00728 | time elapsed: 04h36m05s | time left: 06h12m53s\n",
            "epoch  10 | batch    800 | examples/s:   5.2 | loss: 0.01294 | time elapsed: 04h41m25s | time left: 06h07m10s\n",
            "Training\n",
            "epoch  11 | batch      0 | examples/s:   5.6 | loss: 0.00713 | time elapsed: 04h45m56s | time left: 06h03m54s\n",
            "epoch  11 | batch    200 | examples/s:   5.2 | loss: 0.00589 | time elapsed: 04h51m23s | time left: 05h58m20s\n",
            "epoch  11 | batch    400 | examples/s:   5.3 | loss: 0.00619 | time elapsed: 04h56m44s | time left: 05h52m39s\n",
            "epoch  11 | batch    600 | examples/s:   5.2 | loss: 0.00673 | time elapsed: 05h02m04s | time left: 05h46m57s\n",
            "epoch  11 | batch    800 | examples/s:   4.8 | loss: 0.00757 | time elapsed: 05h07m24s | time left: 05h41m16s\n",
            "Training\n",
            "epoch  12 | batch      0 | examples/s:   4.2 | loss: 0.00844 | time elapsed: 05h11m46s | time left: 05h37m45s\n",
            "epoch  12 | batch    200 | examples/s:   5.2 | loss: 0.00660 | time elapsed: 05h17m23s | time left: 05h32m22s\n",
            "epoch  12 | batch    400 | examples/s:   5.2 | loss: 0.00880 | time elapsed: 05h22m43s | time left: 05h26m41s\n",
            "epoch  12 | batch    600 | examples/s:   5.2 | loss: 0.00755 | time elapsed: 05h28m03s | time left: 05h21m00s\n",
            "epoch  12 | batch    800 | examples/s:   5.1 | loss: 0.00711 | time elapsed: 05h33m22s | time left: 05h15m20s\n",
            "Training\n",
            "epoch  13 | batch      0 | examples/s:   5.3 | loss: 0.00733 | time elapsed: 05h37m43s | time left: 05h11m44s\n",
            "epoch  13 | batch    200 | examples/s:   5.3 | loss: 0.00941 | time elapsed: 05h43m21s | time left: 05h06m21s\n",
            "epoch  13 | batch    400 | examples/s:   5.3 | loss: 0.01400 | time elapsed: 05h48m42s | time left: 05h00m43s\n",
            "epoch  13 | batch    600 | examples/s:   5.3 | loss: 0.00779 | time elapsed: 05h54m05s | time left: 04h55m06s\n",
            "epoch  13 | batch    800 | examples/s:   5.3 | loss: 0.00908 | time elapsed: 05h59m27s | time left: 04h49m29s\n",
            "Training\n",
            "epoch  14 | batch      0 | examples/s:   5.1 | loss: 0.00694 | time elapsed: 06h03m51s | time left: 04h45m53s\n",
            "epoch  14 | batch    200 | examples/s:   5.3 | loss: 0.00665 | time elapsed: 06h09m28s | time left: 04h40m28s\n",
            "epoch  14 | batch    400 | examples/s:   5.2 | loss: 0.00591 | time elapsed: 06h14m48s | time left: 04h34m49s\n",
            "epoch  14 | batch    600 | examples/s:   4.5 | loss: 0.00584 | time elapsed: 06h20m07s | time left: 04h29m11s\n",
            "epoch  14 | batch    800 | examples/s:   5.0 | loss: 0.00460 | time elapsed: 06h25m26s | time left: 04h23m33s\n",
            "Training\n",
            "epoch  15 | batch      0 | examples/s:   5.8 | loss: 0.00878 | time elapsed: 06h29m46s | time left: 04h19m51s\n",
            "epoch  15 | batch    200 | examples/s:   5.2 | loss: 0.00693 | time elapsed: 06h35m24s | time left: 04h14m25s\n",
            "epoch  15 | batch    400 | examples/s:   5.3 | loss: 0.00561 | time elapsed: 06h40m46s | time left: 04h08m50s\n",
            "epoch  15 | batch    600 | examples/s:   5.3 | loss: 0.00715 | time elapsed: 06h46m08s | time left: 04h03m14s\n",
            "epoch  15 | batch    800 | examples/s:   5.1 | loss: 0.00664 | time elapsed: 06h51m29s | time left: 03h57m39s\n",
            "Training\n",
            "epoch  16 | batch      0 | examples/s:   5.1 | loss: 0.00841 | time elapsed: 06h55m58s | time left: 03h53m59s\n",
            "epoch  16 | batch    200 | examples/s:   4.8 | loss: 0.00863 | time elapsed: 07h01m35s | time left: 03h48m32s\n",
            "epoch  16 | batch    400 | examples/s:   4.4 | loss: 0.00542 | time elapsed: 07h07m01s | time left: 03h42m59s\n",
            "epoch  16 | batch    600 | examples/s:   5.1 | loss: 0.00595 | time elapsed: 07h12m29s | time left: 03h37m27s\n",
            "epoch  16 | batch    800 | examples/s:   5.3 | loss: 0.00561 | time elapsed: 07h17m54s | time left: 03h31m54s\n",
            "Training\n",
            "epoch  17 | batch      0 | examples/s:   4.2 | loss: 0.00794 | time elapsed: 07h22m21s | time left: 03h28m10s\n",
            "epoch  17 | batch    200 | examples/s:   5.1 | loss: 0.00855 | time elapsed: 07h28m03s | time left: 03h22m44s\n",
            "epoch  17 | batch    400 | examples/s:   4.9 | loss: 0.00710 | time elapsed: 07h33m30s | time left: 03h17m11s\n",
            "epoch  17 | batch    600 | examples/s:   5.3 | loss: 0.00444 | time elapsed: 07h38m56s | time left: 03h11m38s\n",
            "epoch  17 | batch    800 | examples/s:   5.1 | loss: 0.00977 | time elapsed: 07h44m25s | time left: 03h06m07s\n",
            "Training\n",
            "epoch  18 | batch      0 | examples/s:   5.1 | loss: 0.01166 | time elapsed: 07h48m58s | time left: 03h02m22s\n",
            "epoch  18 | batch    200 | examples/s:   5.1 | loss: 0.00585 | time elapsed: 07h54m42s | time left: 02h56m56s\n",
            "epoch  18 | batch    400 | examples/s:   5.4 | loss: 0.00855 | time elapsed: 08h00m09s | time left: 02h51m23s\n",
            "epoch  18 | batch    600 | examples/s:   4.9 | loss: 0.00777 | time elapsed: 08h05m35s | time left: 02h45m50s\n",
            "epoch  18 | batch    800 | examples/s:   4.5 | loss: 0.00719 | time elapsed: 08h11m01s | time left: 02h40m17s\n",
            "Training\n",
            "epoch  19 | batch      0 | examples/s:   5.3 | loss: 0.00707 | time elapsed: 08h15m31s | time left: 02h36m28s\n",
            "epoch  19 | batch    200 | examples/s:   5.1 | loss: 0.00789 | time elapsed: 08h21m14s | time left: 02h31m00s\n",
            "epoch  19 | batch    400 | examples/s:   5.2 | loss: 0.00775 | time elapsed: 08h26m42s | time left: 02h25m28s\n",
            "epoch  19 | batch    600 | examples/s:   5.1 | loss: 0.00587 | time elapsed: 08h32m07s | time left: 02h19m54s\n",
            "epoch  19 | batch    800 | examples/s:   5.1 | loss: 0.00629 | time elapsed: 08h37m30s | time left: 02h14m20s\n",
            "Training\n",
            "epoch  20 | batch      0 | examples/s:   4.0 | loss: 0.00498 | time elapsed: 08h41m57s | time left: 02h10m29s\n",
            "epoch  20 | batch    200 | examples/s:   4.5 | loss: 0.00548 | time elapsed: 08h47m42s | time left: 02h05m00s\n",
            "epoch  20 | batch    400 | examples/s:   5.3 | loss: 0.00806 | time elapsed: 08h53m08s | time left: 01h59m27s\n",
            "epoch  20 | batch    600 | examples/s:   5.2 | loss: 0.00504 | time elapsed: 08h58m35s | time left: 01h53m54s\n",
            "epoch  20 | batch    800 | examples/s:   5.3 | loss: 0.00598 | time elapsed: 09h04m04s | time left: 01h48m22s\n",
            "Training\n",
            "epoch  21 | batch      0 | examples/s:   5.1 | loss: 0.00601 | time elapsed: 09h08m35s | time left: 01h44m29s\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/AF-SfMLearner/train_end_to_end.py\", line 12, in <module>\n",
            "    trainer.train()\n",
            "  File \"/content/AF-SfMLearner/trainer_end_to_end.py\", line 273, in train\n",
            "    self.run_epoch()\n",
            "  File \"/content/AF-SfMLearner/trainer_end_to_end.py\", line 298, in run_epoch\n",
            "    losses[\"loss\"].backward()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/_tensor.py\", line 625, in backward\n",
            "    torch.autograd.backward(\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/autograd/__init__.py\", line 354, in backward\n",
            "    _engine_run_backward(\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/autograd/graph.py\", line 841, in _engine_run_backward\n",
            "    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "KeyboardInterrupt\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "import os\n",
        "\n",
        "repo_path = \"/content/AF-SfMLearner\"\n",
        "if repo_path not in sys.path:\n",
        "    sys.path.append(repo_path)\n",
        "\n",
        "model_name = \"simcol_sequences_model_320\"\n",
        "log_dir = \"/content/drive/MyDrive/3D_reconstruction/outputs/AF-SfMLearner\"\n",
        "data_path = \"/content/training_data_simcol\"\n",
        "\n",
        "os.makedirs(log_dir, exist_ok=True)\n",
        "\n",
        "print(f\" Training: {model_name}\")\n",
        "print(f\" Data: Sequences  \")\n",
        "print()\n",
        "\n",
        "!cd \"{repo_path}\" && python train_end_to_end.py \\\n",
        "  --data_path \"{data_path}\" \\\n",
        "  --model_name \"{model_name}\" \\\n",
        "  --split \"simcol_sequences\" \\\n",
        "  --dataset \"simcol\" \\\n",
        "  --batch_size 8 \\\n",
        "  --learning_rate 1e-4 \\\n",
        "  --num_epochs 25 \\\n",
        "  --height 320 \\\n",
        "  --scheduler_step_size 10 \\\n",
        "  --width 320 \\\n",
        "  --log_dir \"{log_dir}\" \\\n",
        "  --frame_ids 0 -1 1 \\\n",
        "  --min_depth 1 \\\n",
        "  --max_depth 25 \\\n",
        "  --num_workers 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "inference_header"
      },
      "source": [
        "## 6. Inference (Test S1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tPdiHl-ZCT2e",
        "outputId": "5bf85fc9-1055-4135-a384-9824625785aa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚è≥ ƒêang gi·∫£i n√©n TEST DATA (S1) t·ª´ /content/drive/MyDrive/3D_reconstruction/datasets/FrameBuffer_RGB.zip...\n",
            "üìÇ T√¨m th·∫•y ·∫£nh t·∫°i: /content/dataset_simcol_test/FrameBuffer_RGB\n",
            "‚è≥ ƒêang chu·∫©n h√≥a t√™n file test...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Formatting Test: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1201/1201 [00:00<00:00, 1431.61it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Test data (S1) s·∫µn s√†ng t·∫°i /content/test_data_simcol/simcol_S1_test\n",
            "üìä T·ªïng s·ªë frame: 1201\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import shutil\n",
        "import glob\n",
        "import zipfile\n",
        "from tqdm import tqdm\n",
        "\n",
        "test_zip_path = \"/content/drive/MyDrive/3D_reconstruction/datasets/FrameBuffer_RGB.zip\"\n",
        "test_extract_path = \"/content/dataset_simcol_test\"\n",
        "test_data_root = \"/content/test_data_simcol\"\n",
        "test_sequence_name = \"simcol_S1_test\"\n",
        "test_target_dir = os.path.join(test_data_root, test_sequence_name)\n",
        "\n",
        "if not os.path.exists(test_target_dir):\n",
        "    os.makedirs(test_extract_path, exist_ok=True)\n",
        "    with zipfile.ZipFile(test_zip_path, 'r') as zip_ref:\n",
        "        zip_ref.extractall(test_extract_path)\n",
        "\n",
        "    source_imgs_dir = None\n",
        "    extensions = ['*.png', '*.jpg', '*.jpeg']\n",
        "    for root, dirs, files in os.walk(test_extract_path):\n",
        "        if any(glob.glob(os.path.join(root, ext)) for ext in extensions):\n",
        "            source_imgs_dir = root\n",
        "            break\n",
        "\n",
        "    if not source_imgs_dir:\n",
        "        raise FileNotFoundError(\"No data found!\")\n",
        "\n",
        "    print(f\"Test frames at: {source_imgs_dir}\")\n",
        "\n",
        "    # Format file name\n",
        "    print(\" Correct file test name\")\n",
        "    os.makedirs(test_target_dir, exist_ok=True)\n",
        "    image_files = []\n",
        "    for ext in extensions:\n",
        "        image_files.extend(glob.glob(os.path.join(source_imgs_dir, ext)))\n",
        "    image_files = sorted(image_files)\n",
        "\n",
        "    for idx, file_path in enumerate(tqdm(image_files, desc=\"Formatting Test\")):\n",
        "        shutil.copy(file_path, os.path.join(test_target_dir, \"{:010d}.png\".format(idx)))\n",
        "\n",
        "    print(f\"\\n Test data done: {test_target_dir}\")\n",
        "    print(f\"Frames: {len(image_files)}\")\n",
        "else:\n",
        "    print(\"\\n Test data has exsited.\")\n",
        "    test_files = sorted(glob.glob(os.path.join(test_target_dir, \"*.png\")))\n",
        "    print(f\"Frames: {len(test_files)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "vnzmseg2Dfp4",
        "outputId": "c779bd42-9e44-4ada-f95f-6a73b364f1a0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üöÄ Load Model t·ª´: /content/drive/MyDrive/3D_reconstruction/outputs/AF-SfMLearner/simcol_sequences_model_320/models/weights_20\n",
            "üìÅ Test Data: /content/test_data_simcol/simcol_S1_test\n",
            "--> Loading Encoder...\n",
            "--> Loading Depth Decoder...\n",
            "--> Loading Pose Encoder...\n",
            "--> Loading Pose Decoder...\n",
            "‚è≥ ƒêang x·ª≠ l√Ω 1201 frames t·ª´ TEST SET (S1)...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1201/1201 [01:48<00:00, 11.12it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ ƒê√£ l∆∞u trajectory t·∫°i: /content/inference_output_S1_test/trajectory_S1.txt\n",
            "üéâ K·∫øt qu·∫£ TEST ƒë√£ ƒë∆∞·ª£c l∆∞u v√†o Drive: /content/drive/MyDrive/3D_reconstruction/outputs/AF-SfMLearner/simcol_S1_test_results.zip\n"
          ]
        }
      ],
      "source": [
        "# INFERENCE\n",
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "import glob\n",
        "import os\n",
        "import networks\n",
        "from layers import transformation_from_parameters\n",
        "import shutil\n",
        "\n",
        "# --- CONFIG ---\n",
        "model_name_folder = \"simcol_sequences_model_320\"\n",
        "log_dir = \"/content/drive/MyDrive/3D_reconstruction/outputs/AF-SfMLearner\"\n",
        "\n",
        "best_epoch = 24\n",
        "final_output_dir = os.path.join(log_dir, model_name_folder, \"models\", f\"weights_20\")\n",
        "\n",
        "test_target_dir = \"/content/test_data_simcol/simcol_S1_test\"\n",
        "\n",
        "# Output\n",
        "inference_out = \"/content/inference_output_S1_test\"\n",
        "os.makedirs(inference_out, exist_ok=True)\n",
        "device = torch.device(\"cuda\")\n",
        "\n",
        "infer_height = 320\n",
        "infer_width = 320\n",
        "\n",
        "print(f\"Load Model: {final_output_dir}\")\n",
        "print(f\"Test Data: {test_target_dir}\")\n",
        "\n",
        "def load_weights_safe(model, path):\n",
        "    if not os.path.exists(path):\n",
        "        raise FileNotFoundError(f\"‚ùå Kh√¥ng t√¨m th·∫•y file: {path}\")\n",
        "    loaded_dict = torch.load(path, map_location=device)\n",
        "    model_dict = model.state_dict()\n",
        "    filtered_dict = {k: v for k, v in loaded_dict.items() if k in model_dict}\n",
        "    model.load_state_dict(filtered_dict)\n",
        "    return model\n",
        "\n",
        "# --- LOAD MODELS ---\n",
        "print(\"Loading Encoder\")\n",
        "enc = networks.ResnetEncoder(18, False).to(device)\n",
        "load_weights_safe(enc, os.path.join(final_output_dir, \"encoder.pth\"))\n",
        "\n",
        "print(\"Loading Depth Decoder\")\n",
        "dec = networks.DepthDecoder(enc.num_ch_enc).to(device)\n",
        "load_weights_safe(dec, os.path.join(final_output_dir, \"depth.pth\"))\n",
        "\n",
        "print(\"Loading Pose Encoder\")\n",
        "pose_enc = networks.ResnetEncoder(18, False, num_input_images=2).to(device)\n",
        "load_weights_safe(pose_enc, os.path.join(final_output_dir, \"pose_encoder.pth\"))\n",
        "\n",
        "print(\"Loading Pose Decoder\")\n",
        "pose_dec = networks.PoseDecoder(pose_enc.num_ch_enc, num_input_features=1, num_frames_to_predict_for=2).to(device)\n",
        "load_weights_safe(pose_dec, os.path.join(final_output_dir, \"pose.pth\"))\n",
        "\n",
        "for m in [enc, dec, pose_enc, pose_dec]:\n",
        "    m.eval()\n",
        "\n",
        "# --- INFERENCE ---\n",
        "img_files = sorted(glob.glob(os.path.join(test_target_dir, \"*.png\")))\n",
        "poses = [np.eye(4)]\n",
        "current_pose = np.eye(4)\n",
        "depth_dir = os.path.join(inference_out, \"depth\")\n",
        "os.makedirs(depth_dir, exist_ok=True)\n",
        "\n",
        "with torch.no_grad():\n",
        "    for i in tqdm(range(len(img_files))):\n",
        "        img_t_pil = Image.open(img_files[i]).convert('RGB').resize((infer_width, infer_height), Image.LANCZOS)\n",
        "        img_t = torch.from_numpy(np.array(img_t_pil)).permute(2,0,1).float().unsqueeze(0).to(device) / 255.0\n",
        "\n",
        "        # Predict Depth\n",
        "        disp = dec(enc(img_t))[(\"disp\", 0)]\n",
        "        np.save(os.path.join(depth_dir, f\"{i:010d}.npy\"), disp.cpu().numpy())\n",
        "        plt.imsave(os.path.join(depth_dir, f\"{i:010d}.png\"), disp.squeeze().cpu().numpy(), cmap='magma')\n",
        "\n",
        "        # Predict Pose\n",
        "        if i < len(img_files) - 1:\n",
        "            img_next_pil = Image.open(img_files[i+1]).convert('RGB').resize((infer_width, infer_height), Image.LANCZOS)\n",
        "            img_next = torch.from_numpy(np.array(img_next_pil)).permute(2,0,1).float().unsqueeze(0).to(device) / 255.0\n",
        "\n",
        "            input_cat = torch.cat([img_next, img_t], 1)\n",
        "            features = [pose_enc(input_cat)]\n",
        "            axisangle, translation = pose_dec(features)\n",
        "\n",
        "            pose_mat = transformation_from_parameters(axisangle[:, 0], translation[:, 0]).cpu().numpy()[0]\n",
        "            current_pose = current_pose @ pose_mat\n",
        "            poses.append(current_pose)\n",
        "\n",
        "# --- SAVE TRAJECTORY ---\n",
        "out_path = os.path.join(inference_out, \"trajectory_S1.txt\")\n",
        "with open(out_path, \"w\") as f:\n",
        "    for p in poses:\n",
        "        row_str = \" \".join(map(str, p[:3, :].flatten()))\n",
        "        f.write(row_str + \"\\n\")\n",
        "\n",
        "print(f\" Save trajectory at: {out_path}\")\n",
        "\n",
        "# Zip results\n",
        "zip_name = \"simcol_S1_test_results\"\n",
        "shutil.make_archive(os.path.join(log_dir, zip_name), 'zip', inference_out)\n",
        "print(f\" Save test result: {os.path.join(log_dir, zip_name + '.zip')}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}